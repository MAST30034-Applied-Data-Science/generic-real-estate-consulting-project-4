{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final outputs:\n",
    "\n",
    "#### ERP + household + school_count + PTV -> by sa2\n",
    "SA2_info\n",
    "- ../data/curated/sa2_info.csv\n",
    "\n",
    "#### ERP + price_index + interest_rate + price_index -> year, SA2\n",
    "history_data\n",
    "- ../data/curated/history_data.csv\n",
    "\n",
    "#### Rent data:\n",
    "- ../data/curated/rent.csv\n",
    "\n",
    "#### PTV data\n",
    "- ../data/raw/PTV/public_trans.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/14 22:02:57 WARN Utils: Your hostname, Bruce-PC resolves to a loopback address: 127.0.1.1; using 172.21.205.150 instead (on interface eth0)\n",
      "22/09/14 22:02:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/14 22:02:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/09/14 22:03:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import re\n",
    "from itertools import compress\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import zipfile\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Assignment_2\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "INPUT_DIR = \"../data/raw/\"\n",
    "OUTPUT_DIR = \"../data/curated/\"\n",
    "\n",
    "\n",
    "headers = {\"accept\": \"text/csv\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpd_station_merge(poly_gdf, file_path, by_id_name = \"SA2_CODE21\",\\\n",
    "    station_id_name = \"STOP_ID\", method={\"STOP_ID\": \"count\"}):\n",
    "    \"\"\"\n",
    "        A function used to merge shape file in path: {file_path} to a \n",
    "        geopandas dataframe {poly_gdf} with POLYGON geometry. \n",
    "        poly_gdf: a geopandas.GeoDataFrame object contains POLYGON geometry\n",
    "        file_path: a String of file path to read target shape file\n",
    "        by_id_name: a String of id name to perform groupby option\n",
    "        station_id_name: a String of id name stated in the readed gdf\n",
    "        method: a Dict of operations to perform after groupby\n",
    "    \"\"\"\n",
    "\n",
    "    ### read station file\n",
    "    station_gdf = gpd.read_file(file_path)\n",
    "\n",
    "    # metro bus station feature selection\n",
    "    station_gdf = station_gdf[[station_id_name, \"geometry\"]]\n",
    "    \n",
    "\n",
    "\n",
    "    # merge tabels\n",
    "    join_gdf = gpd.sjoin(poly_gdf, station_gdf, how=\"left\")\n",
    "    join_gdf = join_gdf.groupby(by_id_name).agg(method)\n",
    "    \n",
    "    return join_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SA2_info\n",
    "#### ERP + household + school_count + PTV -> by sa2\n",
    "Read and prepare all data sets and create TempView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------\n",
      " SA2          | 209031212 \n",
      " school_count | 9         \n",
      "only showing top 1 row\n",
      "\n",
      "-RECORD 0---------------\n",
      " SA2        | 201011481 \n",
      " year       | 2021      \n",
      " population | 9656      \n",
      "only showing top 1 row\n",
      "\n",
      "-RECORD 0------------------\n",
      " SA2           | 213051589 \n",
      " median_income | 1862      \n",
      "-RECORD 1------------------\n",
      " SA2           | 209041437 \n",
      " median_income | 1979      \n",
      "only showing top 2 rows\n",
      "\n",
      "-RECORD 0----------------------------------------------------------------------------------------------------------------\n",
      " SA2_CODE21       | 201011001                                                                                            \n",
      " geometry         | POLYGON ((143.78282104711133 -37.566657808073295, 143.75557764214773 -37.56346721632544, 143.7480... \n",
      " metrobus_count   | 0                                                                                                    \n",
      " metrotrain_count | 0                                                                                                    \n",
      " metrotram_count  | 0                                                                                                    \n",
      " regbus_count     | 43                                                                                                   \n",
      " regcoach_count   | 0                                                                                                    \n",
      " regtrain_count   | 0                                                                                                    \n",
      " skybus_count     | 0                                                                                                    \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read school file\n",
    "school_sdf = spark.read.csv(f\"{INPUT_DIR}ACARA/School_location/School_location.csv\", header=True)\n",
    "\n",
    "# count school by SA2\n",
    "school_count = school_sdf.groupBy(\"SA2\").agg({\n",
    "    \"School Name\": \"count\"\n",
    "})\n",
    "school_count = school_count.withColumnRenamed( \"count(School Name)\", \"school_count\")\n",
    "school_count.show(1, vertical = True, truncate=100)\n",
    "school_count.createOrReplaceTempView(\"school\")\n",
    "\n",
    "\n",
    "\n",
    "# read ERP file and create tempview\n",
    "ERP_sdf = spark.read.csv(f\"{INPUT_DIR}ABS/ERP/ERP.csv\", header=True)\n",
    "ERP_sdf = ERP_sdf.filter(F.col(\"year\") == 2021)\n",
    "ERP_sdf.show(1, vertical = True, truncate=100)\n",
    "ERP_sdf.createOrReplaceTempView(\"ERP\")\n",
    "\n",
    "\n",
    "\n",
    "# read median household income file and create tempview\n",
    "household_sdf = spark.read.csv(f\"{INPUT_DIR}ABS/Household_income/Household_income.csv\", header=True)\n",
    "household_sdf.show(2, vertical = True, truncate=100)\n",
    "household_sdf.createOrReplaceTempView(\"household\")\n",
    "\n",
    "# read PTV station file and create tempview\n",
    "ptv_sdf = spark.read.csv(f\"{INPUT_DIR}PTV/public_trans.csv\", header=True)\n",
    "ptv_sdf.show(1, vertical = True, truncate=100)\n",
    "ptv_sdf.createOrReplaceTempView(\"ptv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SA2', 'school_count']\n",
      "['SA2', 'year', 'population']\n",
      "['SA2', 'median_income']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------\n",
      " SA2              | 202011018 \n",
      " school_count     | 13        \n",
      " ERP_population   | 14951     \n",
      " median_income    | 1267      \n",
      " metrobus_count   | 0         \n",
      " metrotrain_count | 0         \n",
      " metrotram_count  | 0         \n",
      " regbus_count     | 142       \n",
      " regcoach_count   | 2         \n",
      " regtrain_count   | 1         \n",
      " skybus_count     | 0         \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inner join\n",
    "print(school_count.columns)\n",
    "print(ERP_sdf.columns)\n",
    "print(household_sdf.columns)\n",
    "combine_sdf = spark.sql(\"\"\"\n",
    "    SELECT  school.SA2, school.school_count, \n",
    "        ERP.population AS ERP_population, median_income, \n",
    "        metrobus_count, metrotrain_count, metrotram_count, \n",
    "        regbus_count, regcoach_count, regtrain_count, skybus_count\n",
    "    FROM school\n",
    "    INNER JOIN ERP ON school.SA2 = ERP.SA2\n",
    "    INNER JOIN household ON school.SA2 = household.SA2\n",
    "    INNER JOIN ptv ON school.SA2 = ptv.SA2_CODE21\n",
    "\"\"\")\n",
    "combine_sdf.show(1, vertical = True, truncate=100)\n",
    "combine_sdf.write.option(\"header\", True).mode(\"overwrite\").csv(\n",
    "    f\"{OUTPUT_DIR}sa2_info.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## history_data\n",
    "\n",
    "#### ERP + interest_rate + price_index -> year, SA2\n",
    "\n",
    "Read and prepare all data sets and create TempView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------\n",
      " SA2        | 201011481 \n",
      " year       | 2021      \n",
      " population | 9656      \n",
      "only showing top 1 row\n",
      "\n",
      "22/09/14 11:43:26 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , year, bond\n",
      " Schema: _c0, year, bond\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/bruce/projects/ass2/data/raw/rba/interest_rate/interest_rate.csv\n",
      "-RECORD 0----\n",
      " _c0  | 112  \n",
      " year | 2021 \n",
      " bond | 0.93 \n",
      "\n",
      "-RECORD 0-----------\n",
      " year        | 2021 \n",
      " price_index | 185  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read ERP file and create tempview\n",
    "ERP_sdf = spark.read.csv(f\"{INPUT_DIR}ABS/ERP/ERP.csv\", header=True)\n",
    "ERP_sdf = ERP_sdf.filter(F.col(\"year\") == 2021)\n",
    "ERP_sdf.show(1, vertical = True, truncate=100)\n",
    "ERP_sdf.createOrReplaceTempView(\"ERP\")\n",
    "\n",
    "# read population projection file and create tempview\n",
    "interest_sdf = spark.read.csv(f\"{INPUT_DIR}rba/interest_rate/interest_rate.csv\", header=True)\n",
    "interest_sdf = interest_sdf.filter(F.col(\"year\") == 2021)\n",
    "interest_sdf.show(1, vertical = True, truncate=100)\n",
    "interest_sdf.createOrReplaceTempView(\"interest\")\n",
    "\n",
    "# read property price index file and create tempview\n",
    "index_sdf = spark.read.csv(f\"{INPUT_DIR}ABS/Price_index/Price_index.csv\", header=True)\n",
    "index_sdf = index_sdf.filter(F.col(\"year\") == 2021)\n",
    "index_sdf.show(1, vertical = True, truncate=100)\n",
    "index_sdf.createOrReplaceTempView(\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SA2', 'year', 'population']\n",
      "['_c0', 'year', 'bond']\n",
      "['year', 'price_index']\n",
      "-RECORD 0----------------\n",
      " SA2         | 201011481 \n",
      " year        | 2021      \n",
      " population  | 9656      \n",
      " bond        | 0.93      \n",
      " price_index | 185       \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# inner join\n",
    "print(ERP_sdf.columns)\n",
    "print(interest_sdf.columns)\n",
    "print(index_sdf.columns)\n",
    "combine_sdf = spark.sql(\"\"\"\n",
    "    SELECT  SA2, ERP.year, population, bond, price_index\n",
    "    FROM ERP\n",
    "    INNER JOIN interest ON ERP.year = interest.year\n",
    "    INNER JOIN index ON ERP.year = index.year\n",
    "\"\"\")\n",
    "combine_sdf.show(1, vertical = True, truncate=100)\n",
    "combine_sdf.write.option(\"header\", True).mode(\"overwrite\").csv(\n",
    "    f\"{OUTPUT_DIR}history_data.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match SA2 to Rental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          rent  bedroom  baths  parking  \\\n",
      "rent_id                                   \n",
      "0        300.0        2      1        1   \n",
      "1        510.0        1      1        1   \n",
      "2        330.0        1      1        1   \n",
      "3        620.0        3      2        1   \n",
      "4        560.0        3      2        1   \n",
      "\n",
      "                                                       url   Latitude  \\\n",
      "rent_id                                                                 \n",
      "0        https://www.domain.com.au/3-180-williamson-str... -36.768411   \n",
      "1        https://www.domain.com.au/3203-118-kavanagh-st... -36.768411   \n",
      "2        https://www.domain.com.au/6-13-laura-street-mo... -36.768411   \n",
      "3        https://www.domain.com.au/14-patterson-street-... -36.768411   \n",
      "4        https://www.domain.com.au/6-gent-street-yarrav... -36.768411   \n",
      "\n",
      "          Longitude                     geometry        SA2  \n",
      "rent_id                                                      \n",
      "0        144.290465  POINT (144.29047 -36.76841)  202011020  \n",
      "1        144.290465  POINT (144.29047 -36.76841)  202011020  \n",
      "2        144.290465  POINT (144.29047 -36.76841)  202011020  \n",
      "3        144.290465  POINT (144.29047 -36.76841)  202011020  \n",
      "4        144.290465  POINT (144.29047 -36.76841)  202011020  \n"
     ]
    }
   ],
   "source": [
    "# read local files\n",
    "rent_df = pd.read_csv(f\"../data/raw/rent/rent_raw.csv\").reset_index()\n",
    "\n",
    "\n",
    "rent_gdf = gpd.GeoDataFrame(rent_df, geometry=gpd\\\n",
    "        .points_from_xy(rent_df[\"Longitude\"], rent_df[\"Latitude\"]))\n",
    "boundary_gdf = gpd.read_file(f\"../data/raw/ABS/digitalBoundary/SA2_2021_AUST_GDA2020.shp\")\n",
    "rent_gdf[\"geometry\"] = rent_gdf[\"geometry\"].set_crs(\"epsg:7844\")\n",
    "\n",
    "# feature selction on boundary gdf\n",
    "boundary_gdf = boundary_gdf.loc[boundary_gdf[\"STE_NAME21\"] == \"Victoria\"]\n",
    "boundary_gdf = boundary_gdf[[\"SA2_CODE21\", \"geometry\"]]\n",
    "boundary_gdf[\"SA2_CODE21\"] = boundary_gdf[\"SA2_CODE21\"].astype(\"int64\")\n",
    "\n",
    "# assgin sa2 to rent gdf\n",
    "join_gdf = gpd.sjoin(rent_gdf, boundary_gdf, how=\"right\")\n",
    "join_gdf = join_gdf.dropna()\n",
    "join_gdf[\"index_left\"] = join_gdf[\"index_left\"].astype(\"int\")\n",
    "rent_gdf = rent_gdf.loc[join_gdf[\"index_left\"]]\n",
    "join_gdf = join_gdf.reset_index()\n",
    "rent_gdf[\"SA2\"] = join_gdf[[\"SA2_CODE21\"]]\n",
    "rent_gdf[\"SA2\"] = rent_gdf[\"SA2\"].astype(\"int64\")\n",
    "rent_df = rent_df.loc[rent_gdf[\"index\"]]\n",
    "rent_df[\"SA2\"] = rent_gdf[\"SA2\"]\n",
    "\n",
    "rent_df[\"rent_id\"] = range(rent_df.shape[0])\n",
    "rent_df = rent_df.set_index(\"rent_id\").drop([\"index\"], axis=1)\n",
    "\n",
    "rent_df = rent_df.dropna().drop_duplicates()\n",
    "print(rent_df.head())\n",
    "rent_df.to_csv(f\"../data/curated/rent.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PTV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../data/raw/PTV/ll_gda2020/esrishape/whole_of_dataset/victoria/PUBLIC_TRANSPORT/\"\n",
    "\n",
    "# unzip zip file\n",
    "with zipfile.ZipFile(f\"../data/raw/PTV.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(f\"../data/raw/PTV/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(524, 1)\n",
      "                                                     geometry\n",
      "SA2_CODE21                                                   \n",
      "201011001   POLYGON ((143.78282 -37.56666, 143.75558 -37.5...\n"
     ]
    }
   ],
   "source": [
    "### read shape file and make geometry readable\n",
    "boundary_gdf = gpd.read_file(\"../data/raw/ABS/digitalBoundary/\\\n",
    "SA2_2021_AUST_GDA2020.shp\")\n",
    "boundary_gdf = boundary_gdf.loc[boundary_gdf[\"STE_NAME21\"] == \"Victoria\"]\n",
    "\n",
    "# digital boundary feature selection\n",
    "boundary_gdf = boundary_gdf.reset_index()[[\"SA2_CODE21\", \"geometry\"]].set_index(\"SA2_CODE21\")\n",
    "print(boundary_gdf.shape)\n",
    "print(boundary_gdf.head(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrobus_count ../data/raw/PTV/ll_gda2020/esrishape/whole_of_dataset/victoria/PUBLIC_TRANSPORT/PTV_METRO_BUS_STOP.shp\n",
      "metrotrain_count ../data/raw/PTV/ll_gda2020/esrishape/whole_of_dataset/victoria/PUBLIC_TRANSPORT/PTV_METRO_TRAIN_STATION.shp\n",
      "metrotram_count ../data/raw/PTV/ll_gda2020/esrishape/whole_of_dataset/victoria/PUBLIC_TRANSPORT/PTV_METRO_TRAM_STOP.shp\n",
      "regbus_count ../data/raw/PTV/ll_gda2020/esrishape/whole_of_dataset/victoria/PUBLIC_TRANSPORT/PTV_REGIONAL_BUS_STOP.shp\n",
      "regcoach_count ../data/raw/PTV/ll_gda2020/esrishape/whole_of_dataset/victoria/PUBLIC_TRANSPORT/PTV_REGIONAL_COACH_STOP.shp\n",
      "regtrain_count ../data/raw/PTV/ll_gda2020/esrishape/whole_of_dataset/victoria/PUBLIC_TRANSPORT/PTV_REGIONAL_TRAIN_STATION.shp\n",
      "skybus_count ../data/raw/PTV/ll_gda2020/esrishape/whole_of_dataset/victoria/PUBLIC_TRANSPORT/PTV_SKYBUS_STOP.shp\n",
      "(524, 8)\n",
      "                                                     geometry  metrobus_count  \\\n",
      "SA2_CODE21                                                                      \n",
      "201011001   POLYGON ((143.78282 -37.56666, 143.75558 -37.5...               0   \n",
      "201011002   POLYGON ((143.81896 -37.55582, 143.81644 -37.5...               0   \n",
      "201011005   POLYGON ((143.84171 -37.61596, 143.84176 -37.6...               0   \n",
      "201011006   POLYGON ((143.75050 -37.59119, 143.75044 -37.5...               0   \n",
      "201011007   POLYGON ((143.73296 -37.62333, 143.73263 -37.6...               0   \n",
      "\n",
      "            metrotrain_count  metrotram_count  regbus_count  regcoach_count  \\\n",
      "SA2_CODE21                                                                    \n",
      "201011001                  0                0            43               0   \n",
      "201011002                  0                0           105               2   \n",
      "201011005                  0                0            32               4   \n",
      "201011006                  0                0            34               2   \n",
      "201011007                  0                0             3               2   \n",
      "\n",
      "            regtrain_count  skybus_count  \n",
      "SA2_CODE21                                \n",
      "201011001                0             0  \n",
      "201011002                0             0  \n",
      "201011005                0             0  \n",
      "201011006                0             0  \n",
      "201011007                0             0  \n"
     ]
    }
   ],
   "source": [
    "mix_gdf = boundary_gdf.sort_values(by=[\"SA2_CODE21\"])\n",
    "file_paths = [\n",
    "    f\"{output_dir}PTV_METRO_BUS_STOP.shp\",\n",
    "    f\"{output_dir}PTV_METRO_TRAIN_STATION.shp\",\n",
    "    f\"{output_dir}PTV_METRO_TRAM_STOP.shp\",\n",
    "    f\"{output_dir}PTV_REGIONAL_BUS_STOP.shp\",\n",
    "    f\"{output_dir}PTV_REGIONAL_COACH_STOP.shp\",\n",
    "    f\"{output_dir}PTV_REGIONAL_TRAIN_STATION.shp\",\n",
    "    f\"{output_dir}PTV_SKYBUS_STOP.shp\"\n",
    "]\n",
    "\n",
    "col_names = [\"metrobus_count\", \"metrotrain_count\", \"metrotram_count\",\n",
    "    \"regbus_count\", \"regcoach_count\", \"regtrain_count\", \"skybus_count\"]\n",
    "\n",
    "\n",
    "for col_name, file_path in zip(col_names, file_paths):\n",
    "    print(col_name, file_path)\n",
    "    cur_gdf = gpd_station_merge(boundary_gdf, file_path).rename({\"STOP_ID\": col_name}, axis=1)\n",
    "    cur_gdf = cur_gdf.sort_values(by = col_name)\n",
    "    mix_gdf = pd.concat([mix_gdf, cur_gdf], axis=1)\n",
    "\n",
    "print(mix_gdf.shape)\n",
    "print(mix_gdf.head())\n",
    "mix_gdf.to_csv(\"../data/raw/PTV/public_trans.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
