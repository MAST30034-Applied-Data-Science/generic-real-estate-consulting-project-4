{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final outputs:\n",
    "\n",
    "#### ERP + household + school_count + PTV -> by sa2\n",
    "SA2_info\n",
    "- ../data/curated/sa2_info.csv\n",
    "\n",
    "#### ERP + price_index + interest_rate + price_index -> year, SA2\n",
    "history_data\n",
    "- ../data/curated/history_data.csv\n",
    "\n",
    "#### Rent data:\n",
    "- ../data/curated/rent.csv\n",
    "\n",
    "#### PTV data\n",
    "- ../data/raw/PTV/public_trans.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import re\n",
    "from itertools import compress\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import zipfile\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Assignment_2\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "INPUT_DIR = \"../data/raw/\"\n",
    "OUTPUT_DIR = \"../data/curated/\"\n",
    "\n",
    "\n",
    "headers = {\"accept\": \"text/csv\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def gpd_station_merge(poly_gdf, file_path, by_id_name = \"SA2_CODE21\",\\\n",
    "    station_id_name = \"STOP_ID\", method={\"STOP_ID\": \"count\"}):\n",
    "    \"\"\"\n",
    "        A function used to merge shape file in path: {file_path} to a \n",
    "        geopandas dataframe {poly_gdf} with POLYGON geometry. \n",
    "        poly_gdf: a geopandas.GeoDataFrame object contains POLYGON geometry\n",
    "        file_path: a String of file path to read target shape file\n",
    "        by_id_name: a String of id name to perform groupby option\n",
    "        station_id_name: a String of id name stated in the readed gdf\n",
    "        method: a Dict of operations to perform after groupby\n",
    "    \"\"\"\n",
    "\n",
    "    ### read station file\n",
    "    station_gdf = gpd.read_file(file_path)\n",
    "\n",
    "    # metro bus station feature selection\n",
    "    station_gdf = station_gdf[[station_id_name, \"geometry\"]]\n",
    "    \n",
    "\n",
    "\n",
    "    # merge tabels\n",
    "    join_gdf = gpd.sjoin(poly_gdf, station_gdf, how=\"left\")\n",
    "    join_gdf = join_gdf.groupby(by_id_name).agg(method)\n",
    "    \n",
    "    return join_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PTV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "output_dir = \"../data/raw/PTV/ll_gda2020/esrishape/whole_of_dataset/victoria/PUBLIC_TRANSPORT/\"\n",
    "\n",
    "# unzip zip file\n",
    "with zipfile.ZipFile(f\"../data/raw/PTV.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(f\"../data/raw/PTV/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(524, 1)\n",
      "                                                     geometry\n",
      "SA2_CODE21                                                   \n",
      "201011001   POLYGON ((143.78282 -37.56666, 143.75558 -37.5...\n"
     ]
    }
   ],
   "source": [
    "### read shape file and make geometry readable\n",
    "boundary_gdf = gpd.read_file(\"../data/raw/ABS/digitalBoundary/\\\n",
    "SA2_2021_AUST_GDA2020.shp\")\n",
    "boundary_gdf = boundary_gdf.loc[boundary_gdf[\"STE_NAME21\"] == \"Victoria\"]\n",
    "\n",
    "# digital boundary feature selection\n",
    "boundary_gdf = boundary_gdf.reset_index()[[\"SA2_CODE21\", \"geometry\"]].set_index(\"SA2_CODE21\")\n",
    "print(boundary_gdf.shape)\n",
    "print(boundary_gdf.head(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrobus_count ../data/raw/PTV/ll_gda2020/esrishape/whole_of_dataset/victoria/PUBLIC_TRANSPORT/PTV_METRO_BUS_STOP.shp\n",
      "metrotrain_count ../data/raw/PTV/ll_gda2020/esrishape/whole_of_dataset/victoria/PUBLIC_TRANSPORT/PTV_METRO_TRAIN_STATION.shp\n",
      "metrotram_count ../data/raw/PTV/ll_gda2020/esrishape/whole_of_dataset/victoria/PUBLIC_TRANSPORT/PTV_METRO_TRAM_STOP.shp\n",
      "regbus_count ../data/raw/PTV/ll_gda2020/esrishape/whole_of_dataset/victoria/PUBLIC_TRANSPORT/PTV_REGIONAL_BUS_STOP.shp\n",
      "regcoach_count ../data/raw/PTV/ll_gda2020/esrishape/whole_of_dataset/victoria/PUBLIC_TRANSPORT/PTV_REGIONAL_COACH_STOP.shp\n",
      "regtrain_count ../data/raw/PTV/ll_gda2020/esrishape/whole_of_dataset/victoria/PUBLIC_TRANSPORT/PTV_REGIONAL_TRAIN_STATION.shp\n",
      "skybus_count ../data/raw/PTV/ll_gda2020/esrishape/whole_of_dataset/victoria/PUBLIC_TRANSPORT/PTV_SKYBUS_STOP.shp\n",
      "(524, 8)\n",
      "                                                     geometry  metrobus_count  \\\n",
      "SA2_CODE21                                                                      \n",
      "201011001   POLYGON ((143.78282 -37.56666, 143.75558 -37.5...               0   \n",
      "201011002   POLYGON ((143.81896 -37.55582, 143.81644 -37.5...               0   \n",
      "201011005   POLYGON ((143.84171 -37.61596, 143.84176 -37.6...               0   \n",
      "201011006   POLYGON ((143.75050 -37.59119, 143.75044 -37.5...               0   \n",
      "201011007   POLYGON ((143.73296 -37.62333, 143.73263 -37.6...               0   \n",
      "\n",
      "            metrotrain_count  metrotram_count  regbus_count  regcoach_count  \\\n",
      "SA2_CODE21                                                                    \n",
      "201011001                  0                0            43               0   \n",
      "201011002                  0                0           105               2   \n",
      "201011005                  0                0            32               4   \n",
      "201011006                  0                0            34               2   \n",
      "201011007                  0                0             3               2   \n",
      "\n",
      "            regtrain_count  skybus_count  \n",
      "SA2_CODE21                                \n",
      "201011001                0             0  \n",
      "201011002                0             0  \n",
      "201011005                0             0  \n",
      "201011006                0             0  \n",
      "201011007                0             0  \n"
     ]
    }
   ],
   "source": [
    "mix_gdf = boundary_gdf.sort_values(by=[\"SA2_CODE21\"])\n",
    "file_paths = [\n",
    "    f\"{output_dir}PTV_METRO_BUS_STOP.shp\",\n",
    "    f\"{output_dir}PTV_METRO_TRAIN_STATION.shp\",\n",
    "    f\"{output_dir}PTV_METRO_TRAM_STOP.shp\",\n",
    "    f\"{output_dir}PTV_REGIONAL_BUS_STOP.shp\",\n",
    "    f\"{output_dir}PTV_REGIONAL_COACH_STOP.shp\",\n",
    "    f\"{output_dir}PTV_REGIONAL_TRAIN_STATION.shp\",\n",
    "    f\"{output_dir}PTV_SKYBUS_STOP.shp\"\n",
    "]\n",
    "\n",
    "col_names = [\"metrobus_count\", \"metrotrain_count\", \"metrotram_count\",\n",
    "    \"regbus_count\", \"regcoach_count\", \"regtrain_count\", \"skybus_count\"]\n",
    "\n",
    "\n",
    "for col_name, file_path in zip(col_names, file_paths):\n",
    "    print(col_name, file_path)\n",
    "    cur_gdf = gpd_station_merge(boundary_gdf, file_path).rename({\"STOP_ID\": col_name}, axis=1)\n",
    "    cur_gdf = cur_gdf.sort_values(by = col_name)\n",
    "    mix_gdf = pd.concat([mix_gdf, cur_gdf], axis=1)\n",
    "\n",
    "print(mix_gdf.shape)\n",
    "print(mix_gdf.head())\n",
    "mix_gdf.to_csv(\"../data/raw/PTV/public_trans.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SA2_info\n",
    "#### ERP + household + school_count + PTV -> by sa2\n",
    "Read and prepare all data sets and create TempView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------\n",
      " SA2          | 209031212 \n",
      " school_count | 9         \n",
      "only showing top 1 row\n",
      "\n",
      "-RECORD 0---------------\n",
      " SA2        | 201011481 \n",
      " year       | 2021      \n",
      " population | 9656      \n",
      "only showing top 1 row\n",
      "\n",
      "-RECORD 0------------------\n",
      " SA2           | 213051589 \n",
      " median_income | 1862      \n",
      "-RECORD 1------------------\n",
      " SA2           | 209041437 \n",
      " median_income | 1979      \n",
      "only showing top 2 rows\n",
      "\n",
      "-RECORD 0----------------------------------------------------------------------------------------------------------------\n",
      " SA2_CODE21       | 201011001                                                                                            \n",
      " geometry         | POLYGON ((143.78282104711133 -37.566657808073295, 143.75557764214773 -37.56346721632544, 143.7480... \n",
      " metrobus_count   | 0                                                                                                    \n",
      " metrotrain_count | 0                                                                                                    \n",
      " metrotram_count  | 0                                                                                                    \n",
      " regbus_count     | 43                                                                                                   \n",
      " regcoach_count   | 0                                                                                                    \n",
      " regtrain_count   | 0                                                                                                    \n",
      " skybus_count     | 0                                                                                                    \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read school file\n",
    "# school_sdf = spark.read.csv(f\"{INPUT_DIR}ACARA/School_location/School_location.csv\", header=True)\n",
    "school_sdf = spark.read.csv(f\"{INPUT_DIR}ACARA/School_location.csv\", header=True)\n",
    "# count school by SA2\n",
    "school_count = school_sdf.groupBy(\"SA2\").agg({\n",
    "    \"School Name\": \"count\"\n",
    "})\n",
    "school_count = school_count.withColumnRenamed( \"count(School Name)\", \"school_count\")\n",
    "school_count.show(1, vertical = True, truncate=100)\n",
    "school_count.createOrReplaceTempView(\"school\")\n",
    "\n",
    "\n",
    "\n",
    "# read ERP file and create tempview\n",
    "ERP_sdf = spark.read.csv(f\"{INPUT_DIR}ABS/ERP/ERP.csv\", header=True)\n",
    "ERP_sdf = ERP_sdf.filter(F.col(\"year\") == 2021)\n",
    "ERP_sdf.show(1, vertical = True, truncate=100)\n",
    "ERP_sdf.createOrReplaceTempView(\"ERP\")\n",
    "\n",
    "\n",
    "\n",
    "# read median household income file and create tempview\n",
    "household_sdf = spark.read.csv(f\"{INPUT_DIR}ABS/Household_income/Household_income.csv\", header=True)\n",
    "household_sdf.show(2, vertical = True, truncate=100)\n",
    "household_sdf.createOrReplaceTempView(\"household\")\n",
    "\n",
    "# read PTV station file and create tempview\n",
    "ptv_sdf = spark.read.csv(f\"{INPUT_DIR}PTV/public_trans.csv\", header=True)\n",
    "ptv_sdf.show(1, vertical = True, truncate=100)\n",
    "ptv_sdf.createOrReplaceTempView(\"ptv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SA2', 'school_count']\n",
      "['SA2', 'year', 'population']\n",
      "['SA2', 'median_income']\n",
      "-RECORD 0---------------------\n",
      " SA2              | 202011018 \n",
      " school_count     | 13        \n",
      " ERP_population   | 14951     \n",
      " median_income    | 1267      \n",
      " metrobus_count   | 0         \n",
      " metrotrain_count | 0         \n",
      " metrotram_count  | 0         \n",
      " regbus_count     | 142       \n",
      " regcoach_count   | 2         \n",
      " regtrain_count   | 1         \n",
      " skybus_count     | 0         \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inner join\n",
    "print(school_count.columns)\n",
    "print(ERP_sdf.columns)\n",
    "print(household_sdf.columns)\n",
    "combine_sdf = spark.sql(\"\"\"\n",
    "    SELECT  school.SA2, school.school_count, \n",
    "        ERP.population AS ERP_population, median_income, \n",
    "        metrobus_count, metrotrain_count, metrotram_count, \n",
    "        regbus_count, regcoach_count, regtrain_count, skybus_count\n",
    "    FROM school\n",
    "    INNER JOIN ERP ON school.SA2 = ERP.SA2\n",
    "    INNER JOIN household ON school.SA2 = household.SA2\n",
    "    INNER JOIN ptv ON school.SA2 = ptv.SA2_CODE21\n",
    "\"\"\")\n",
    "combine_sdf.show(1, vertical = True, truncate=100)\n",
    "combine_sdf.write.option(\"header\", True).mode(\"overwrite\").csv(\n",
    "    f\"{OUTPUT_DIR}sa2_info.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## history_data\n",
    "\n",
    "#### ERP + interest_rate + price_index -> year, SA2\n",
    "\n",
    "Read and prepare all data sets and create TempView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------\n",
      " SA2        | 201011481 \n",
      " year       | 2021      \n",
      " population | 9656      \n",
      "only showing top 1 row\n",
      "\n",
      "-RECORD 0----\n",
      " _c0  | 112  \n",
      " year | 2021 \n",
      " bond | 0.93 \n",
      "\n",
      "-RECORD 0-----------\n",
      " year        | 2021 \n",
      " price_index | 185  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read ERP file and create tempview\n",
    "ERP_sdf = spark.read.csv(f\"{INPUT_DIR}ABS/ERP/ERP.csv\", header=True)\n",
    "ERP_sdf = ERP_sdf.filter(F.col(\"year\") == 2021)\n",
    "ERP_sdf.show(1, vertical = True, truncate=100)\n",
    "ERP_sdf.createOrReplaceTempView(\"ERP\")\n",
    "\n",
    "# read population projection file and create tempview\n",
    "interest_sdf = spark.read.csv(f\"{INPUT_DIR}rba/interest_rate/interest_rate.csv\", header=True)\n",
    "interest_sdf = interest_sdf.filter(F.col(\"year\") == 2021)\n",
    "interest_sdf.show(1, vertical = True, truncate=100)\n",
    "interest_sdf.createOrReplaceTempView(\"interest\")\n",
    "\n",
    "# read property price index file and create tempview\n",
    "index_sdf = spark.read.csv(f\"{INPUT_DIR}ABS/Price_index/Price_index.csv\", header=True)\n",
    "index_sdf = index_sdf.filter(F.col(\"year\") == 2021)\n",
    "index_sdf.show(1, vertical = True, truncate=100)\n",
    "index_sdf.createOrReplaceTempView(\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SA2', 'year', 'population']\n",
      "['_c0', 'year', 'bond']\n",
      "['year', 'price_index']\n",
      "-RECORD 0----------------\n",
      " SA2         | 201011481 \n",
      " year        | 2021      \n",
      " population  | 9656      \n",
      " bond        | 0.93      \n",
      " price_index | 185       \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# inner join\n",
    "print(ERP_sdf.columns)\n",
    "print(interest_sdf.columns)\n",
    "print(index_sdf.columns)\n",
    "combine_sdf = spark.sql(\"\"\"\n",
    "    SELECT  SA2, ERP.year, population, bond, price_index\n",
    "    FROM ERP\n",
    "    INNER JOIN interest ON ERP.year = interest.year\n",
    "    INNER JOIN index ON ERP.year = index.year\n",
    "\"\"\")\n",
    "combine_sdf.show(1, vertical = True, truncate=100)\n",
    "combine_sdf.write.option(\"header\", True).mode(\"overwrite\").csv(\n",
    "    f\"{OUTPUT_DIR}history_data.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match SA2 to Rental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rent_df = pd.read_csv(f\"../data/raw/rent/rent_raw.csv\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          rent  bedroom  baths  parking  \\\n",
      "rent_id                                   \n",
      "0        490.0        4      2        2   \n",
      "1        420.0        4      2        2   \n",
      "2        520.0        4      2        2   \n",
      "3        440.0        4      2        2   \n",
      "4        440.0        4      2        2   \n",
      "\n",
      "                                                       url   Latitude  \\\n",
      "rent_id                                                                 \n",
      "0        https://www.domain.com.au/9-kilkenny-drive-alf... -37.563073   \n",
      "1        https://www.domain.com.au/164-shortridge-drive... -37.547241   \n",
      "2        https://www.domain.com.au/37-mullingar-drive-a... -37.566319   \n",
      "3        https://www.domain.com.au/66-lugano-avenue-alf... -37.563453   \n",
      "4        https://www.domain.com.au/57-dyson-drive-alfre... -37.550549   \n",
      "\n",
      "          Longitude                     geometry        SA2  \n",
      "rent_id                                                      \n",
      "0        143.793875  POINT (143.79387 -37.56307)  206021110  \n",
      "1        143.770106  POINT (143.77011 -37.54724)  206051134  \n",
      "2        143.800328  POINT (143.80033 -37.56632)  206051514  \n",
      "3        143.789489  POINT (143.78949 -37.56345)  206071517  \n",
      "4        143.786038  POINT (143.78604 -37.55055)  207011520  \n",
      "(3748, 9)\n"
     ]
    }
   ],
   "source": [
    "# read local files\n",
    "rent_df = pd.read_csv(f\"../data/raw/rent/rent_raw.csv\").reset_index()\n",
    "\n",
    "\n",
    "rent_gdf = gpd.GeoDataFrame(rent_df, geometry=gpd\\\n",
    "        .points_from_xy(rent_df[\"Longitude\"], rent_df[\"Latitude\"]))\n",
    "boundary_gdf = gpd.read_file(f\"../data/raw/ABS/digitalBoundary/SA2_2021_AUST_GDA2020.shp\")\n",
    "rent_gdf[\"geometry\"] = rent_gdf[\"geometry\"].set_crs(\"epsg:7844\")\n",
    "\n",
    "# feature selction on boundary gdf\n",
    "boundary_gdf = boundary_gdf.loc[boundary_gdf[\"STE_NAME21\"] == \"Victoria\"]\n",
    "boundary_gdf = boundary_gdf[[\"SA2_CODE21\", \"geometry\"]]\n",
    "boundary_gdf[\"SA2_CODE21\"] = boundary_gdf[\"SA2_CODE21\"].astype(\"int64\")\n",
    "\n",
    "# assgin sa2 to rent gdf\n",
    "join_gdf = gpd.sjoin(rent_gdf, boundary_gdf, how=\"right\")\n",
    "join_gdf = join_gdf.dropna()\n",
    "join_gdf[\"index_left\"] = join_gdf[\"index_left\"].astype(\"int\")\n",
    "rent_gdf = rent_gdf.loc[join_gdf[\"index_left\"]]\n",
    "join_gdf = join_gdf.reset_index()\n",
    "rent_gdf[\"SA2\"] = join_gdf[[\"SA2_CODE21\"]]\n",
    "rent_gdf = rent_gdf.dropna()\n",
    "rent_gdf[\"SA2\"] = rent_gdf[\"SA2\"].astype(\"int64\")\n",
    "rent_df = rent_df.loc[rent_gdf[\"index\"]]\n",
    "rent_df[\"SA2\"] = rent_gdf[\"SA2\"]\n",
    "\n",
    "rent_df[\"rent_id\"] = range(rent_df.shape[0])\n",
    "rent_df = rent_df.set_index(\"rent_id\").drop([\"index\"], axis=1)\n",
    "\n",
    "rent_df = rent_df.dropna().drop_duplicates()\n",
    "print(rent_df.head())\n",
    "print(rent_df.shape)\n",
    "rent_df.to_csv(f\"../data/curated/rent.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3748, 3)\n"
     ]
    }
   ],
   "source": [
    "print(rent_df[[\"SA2\", \"Latitude\", \"Longitude\"]].drop_duplicates().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_sa2_to_data(target_df, output_path):\n",
    "    # get GeoDataFrame\n",
    "    target_gdf = gpd.GeoDataFrame(target_df, geometry=gpd.GeoSeries.from_wkt(target_df[\"geometry\"]), crs=\"epsg:7844\")\n",
    "\n",
    "    # assgin sa2 to rent gdf\n",
    "    join_gdf = gpd.sjoin(target_gdf, boundary_gdf, how=\"right\")\n",
    "    join_gdf = join_gdf.dropna()\n",
    "    join_gdf[\"index_left\"] = join_gdf[\"index_left\"].astype(\"int\")\n",
    "    target_gdf = target_gdf.loc[join_gdf[\"index_left\"]]\n",
    "    join_gdf = join_gdf.reset_index()\n",
    "    target_gdf[\"SA2\"] = join_gdf[[\"SA2_CODE21\"]].astype(\"int64\")\n",
    "    target_gdf = target_gdf.dropna().drop_duplicates().reset_index(drop=True)\n",
    "    print(target_gdf.head())\n",
    "    print(target_gdf.shape)\n",
    "    target_gdf.to_csv(output_path, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Match SA2 to tram_stop data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      geometry                                 tram_stop  \\\n",
      "0  POINT (144.95357 -37.82154)  Victoria Police Centre / Flinders Street   \n",
      "1  POINT (144.95110 -37.82190)           The Goods Shed / Wurundjeri Way   \n",
      "2  POINT (144.94773 -37.82216)        Docklands Park / Harbour Esplanade   \n",
      "3  POINT (144.94651 -37.81866)         Bourke Street / Harbour Esplanade   \n",
      "4  POINT (144.94512 -37.81543)          Central Pier / Harbour Esplanade   \n",
      "\n",
      "         SA2  \n",
      "0  206041118  \n",
      "1  206041118  \n",
      "2  206041118  \n",
      "3  206041119  \n",
      "4  206041119  \n",
      "(28, 3)\n"
     ]
    }
   ],
   "source": [
    "tram_stop_df = pd.read_csv(f\"../data/raw/tram_stop.csv\", header=None)\n",
    "tram_stop_df = tram_stop_df[[0, 3]]\n",
    "tram_stop_df.columns = [\"geometry\", \"tram_stop\"]\n",
    "tram_stop_outpath = \"../data/curated/tram_stop_sa2.csv\"\n",
    "\n",
    "match_sa2_to_data(tram_stop_df, tram_stop_outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match SA2 to station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     station                     geometry        SA2\n",
      "0     Anstey  POINT (144.96056 -37.76190)  206011497\n",
      "1  Brunswick  POINT (144.95956 -37.76794)  206041127\n",
      "2     Jewell  POINT (144.95875 -37.77485)  208041194\n",
      "3     Coburg  POINT (144.96335 -37.74241)  206061138\n",
      "4   Moreland  POINT (144.96185 -37.75434)  210031236\n",
      "(219, 3)\n"
     ]
    }
   ],
   "source": [
    "station_df = pd.read_csv(f\"../data/raw/station.csv\", header=None)\n",
    "station_df = station_df[[0, 1]]\n",
    "station_df.columns = [\"station\", \"geometry\"]\n",
    "station_outpath = \"../data/curated/station_sa2.csv\"\n",
    "\n",
    "match_sa2_to_data(station_df, station_outpath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match SA2 to parking_spot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            geometry  \\\n",
      "0  MULTIPOLYGON (((144.97048 -37.79701, 144.97046...   \n",
      "1  MULTIPOLYGON (((144.96041 -37.80303, 144.96040...   \n",
      "2  MULTIPOLYGON (((144.97418 -37.79577, 144.97419...   \n",
      "3  MULTIPOLYGON (((144.96450 -37.80544, 144.96449...   \n",
      "4  MULTIPOLYGON (((144.96220 -37.80276, 144.96219...   \n",
      "\n",
      "                                        parking_spot          SA2  \n",
      "0  Rathdowne Street between Elgin Street and Palm...  206041117.0  \n",
      "1  Leicester Street between Queensberry Street an...  206041117.0  \n",
      "2  Station Street between Palmerston Street and K...  206041117.0  \n",
      "3  Cardigan Street between Victoria Street and Qu...  206041117.0  \n",
      "4  Bouverie Street between Lincoln Square South a...  206041117.0  \n",
      "(22573, 3)\n"
     ]
    }
   ],
   "source": [
    "parking_spot_df = pd.read_csv(f\"../data/raw/parking_spot.csv\", header=None)\n",
    "parking_spot_df = parking_spot_df[[0, 6]]\n",
    "parking_spot_df.columns = [\"geometry\", \"parking_spot\"]\n",
    "parking_spot_outpath = \"../data/curated/parking_spot_sa2.csv\"\n",
    "\n",
    "match_sa2_to_data(parking_spot_df, parking_spot_outpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
