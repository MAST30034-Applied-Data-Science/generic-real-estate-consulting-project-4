{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERP + household + school_count + PTV + median_rent + cbd_dis-> by sa2\n",
    "##### INPUT:\n",
    "School location\n",
    "- ../data/raw/ACARA/School_location/School_location.csv\n",
    "- School Name (String), SA2 (int), Latitude (float), Longitude (float), School Type (String)\n",
    "\n",
    "Estimated Resident Population (ERP) (2001 to 2021) (By SA2)\n",
    "- ../data/raw/ABS/ERP/ERP.csv\n",
    "- SA2 (int), year (int), population (int)\n",
    "\n",
    "Median household income (weekly) (2021) (By SA2)\n",
    "- ../data/raw/ABS/Household_income/Household_income.csv\n",
    "- SA2 (int), year (int),  household_type (String), income_level (String), popultaion (int)\n",
    "\n",
    "School location\n",
    "- ../data/raw/ACARA/School_location/School_location.csv\n",
    "- School Name (String), SA2 (int), Latitude (float), Longitude (float), School Type (String)\n",
    "\n",
    "Median rent\n",
    "- ../data/raw/DHHS/history_rent.csv\n",
    "- SA2 (int), year (int), month (int), count (int), median (float)\n",
    "\n",
    "distance to cbd\n",
    "- ../data/curated/sa2_to_cbd.csv\n",
    "- SA2 (int), cbd_dis (float)\n",
    "##### OUTPUT:\n",
    "SA2_info\n",
    "- ../data/curated/sa2_info.csv\n",
    "- SA2 (int), school_count (int), ERP_population (int), median_income (float), metrobus_count (int), metrotrain_count (int), metrotram_count (int), regbus_count (int), regcoach_count (int), regtrain_count (int), skybus_count (int), recr_count (int), comm_count (int), cbd_dis(float)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ERP + price_index + interest_rate + median_rent + exchange_rate + immigration + debt_income_ratio -> BY year and SA2\n",
    "##### INPUT:\n",
    "ERP\n",
    "- ../data/raw/ABS/ERP/ERP.csv\n",
    "- SA2 (int), year (int), population (int)\n",
    "\n",
    "Residential Property Price Index (1/2011 - 12/2021) (AUS)\n",
    "- ../data/raw/ABS/Price_index/Price_index.csv\n",
    "- year (int), quarter (int), price_index (int)\n",
    "\n",
    "Interest rate (2013 - 2021) (AUS)\n",
    "- ../data/raw/rba/interest_rate/interest_rate.csv\n",
    "- year(int), quarter(int), month(int), bond (float, risk free interest rate)\n",
    "\n",
    "Median rent (1999 - 2021) BY sa2\n",
    "- ../data/raw/DHHS/history_rent.csv\n",
    "- SA2 (int), year (int), quarter (int), count (int), median (float)\n",
    "\n",
    "Exchange Rate (2010 March - 2022 June) AUS\n",
    "- ../data/raw/rba/exchange_rate/exchange_rate.csv\n",
    "- year (int), quarter (int), month (int), to_USD (float)\n",
    "\n",
    "Immigration data, 2004 - 2019, Victoria only\n",
    "- ../data/raw/ABS/immigration/immigration.csv\n",
    "- year (int), immi_count (int)\n",
    "\n",
    "Household debt income ratio measured on each two years In Millions (2009-2019) (Victoria only)\n",
    "- ../data/raw/ABS/debt_income_ratio/debt_income_ratio.csv\n",
    "- year (int), debt_ratio (float)\n",
    "\n",
    "##### OUTPUT:\n",
    "history_info\n",
    "- ../data/curated/history_info.csv\n",
    "- SA2 (int), year (int), quarter (int), month (int), bond (float), price_index (int), population (int), median_rent (float), deal_count (int), to_USD (float), immi_count (int), debt_ratio (float)\n",
    "- ERP + price_index + interest_rate + median_rent + exchange_rate + immigration + debt_income_ratio\n",
    "- year: ERP, immigration, debt_ratio (2 years)\n",
    "- quarter: price_index, median_rent\n",
    "- month: interest_rate, exchange_rate\n",
    "\n",
    "\n",
    "# SA2(ERP + household + school_count + PTV + FOI + Median_rent + cbd_distance) + rent(basic + distance) -> rent_price\n",
    "##### INPUT:\n",
    "SA2:\n",
    "- ALL: ../data/curated/sa2_info.csv\n",
    "\n",
    "rent information with distance\n",
    "- ../data/curated/rent_distance.csv\n",
    "- rent_index (int), SA2 (int), rent (float), bedroom (int), baths (int), parking (int), Latitude (float), Longitude (float), school_dis (float), station_dis (float)\n",
    "\n",
    "##### OUTPUT:\n",
    "- ../data/curated/rent_info.csv\n",
    "- rent (float), bedroom (int), baths (int), parking (int), school_dis (float), station_dis (float), cbd_dis (float), median_rent (float), SA2 (int), school_count (int), ERP_population (int), median_income (float), metrobus_count (int), metrotrain_count (int), metrotram_count (int), regbus_count (int), regcoach_count (int), regtrain_count (int), skybus_count (int), recr_count (int), comm_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import re\n",
    "from itertools import compress\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import zipfile\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Assignment_2\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "headers = {\"accept\": \"text/csv\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def gpd_station_merge(poly_gdf, file_path, by_id_name = \"SA2_CODE21\",\\\n",
    "    station_id_name = \"STOP_ID\", method={\"STOP_ID\": \"count\"}):\n",
    "    \"\"\"\n",
    "        A function used to merge shape file in path: {file_path} to a \n",
    "        geopandas dataframe {poly_gdf} with POLYGON geometry. \n",
    "        poly_gdf: a geopandas.GeoDataFrame object contains POLYGON geometry\n",
    "        file_path: a String of file path to read target shape file\n",
    "        by_id_name: a String of id name to perform groupby option\n",
    "        station_id_name: a String of id name stated in the readed gdf\n",
    "        method: a Dict of operations to perform after groupby\n",
    "    \"\"\"\n",
    "\n",
    "    ### read station file\n",
    "    station_gdf = gpd.read_file(file_path)\n",
    "\n",
    "    # metro bus station feature selection\n",
    "    station_gdf = station_gdf[[station_id_name, \"geometry\"]]\n",
    "    \n",
    "\n",
    "\n",
    "    # merge tabels\n",
    "    join_gdf = gpd.sjoin(poly_gdf, station_gdf, how=\"left\")\n",
    "    join_gdf = join_gdf.groupby(by_id_name).agg(method)\n",
    "    \n",
    "    return join_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SA2_info\n",
    "#### ERP + household + school_count + PTV + median_rent-> by sa2\n",
    "Read and prepare all data sets and create TempView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------\n",
      " SA2          | 209031212 \n",
      " school_count | 9         \n",
      "only showing top 1 row\n",
      "\n",
      "-RECORD 0---------------\n",
      " SA2        | 201011481 \n",
      " year       | 2021      \n",
      " population | 9656      \n",
      "only showing top 1 row\n",
      "\n",
      "-RECORD 0------------------\n",
      " SA2           | 213051589 \n",
      " median_income | 1862      \n",
      "-RECORD 1------------------\n",
      " SA2           | 209041437 \n",
      " median_income | 1979      \n",
      "only showing top 2 rows\n",
      "\n",
      "-RECORD 0----------------------------------------------------------------------------------------------------------------\n",
      " SA2_CODE21       | 201011001                                                                                            \n",
      " geometry         | POLYGON ((143.78282104711133 -37.566657808073295, 143.75557764214773 -37.56346721632544, 143.7480... \n",
      " metrobus_count   | 0                                                                                                    \n",
      " metrotrain_count | 0                                                                                                    \n",
      " metrotram_count  | 0                                                                                                    \n",
      " regbus_count     | 43                                                                                                   \n",
      " regcoach_count   | 0                                                                                                    \n",
      " regtrain_count   | 0                                                                                                    \n",
      " skybus_count     | 0                                                                                                    \n",
      "only showing top 1 row\n",
      "\n",
      "-RECORD 0---------------\n",
      " SA2        | 201011001 \n",
      " recr_count | 0         \n",
      " comm_count | 0         \n",
      "only showing top 1 row\n",
      "\n",
      "-RECORD 0------------\n",
      " index   | 86        \n",
      " SA2     | 201011001 \n",
      " year    | 2020      \n",
      " quarter | 4         \n",
      " count   | 1023      \n",
      " median  | 367.5     \n",
      "only showing top 1 row\n",
      "\n",
      "-RECORD 0------------\n",
      " _c0     | 0         \n",
      " SA2     | 201011001 \n",
      " cbd_dis | 350.0     \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read school file\n",
    "school_sdf = spark.read.csv(f\"../data/raw/ACARA/School_location.csv\", header=True)\n",
    "\n",
    "# count school by SA2\n",
    "school_count = school_sdf.groupBy(\"SA2\").agg({\n",
    "    \"School Name\": \"count\"\n",
    "})\n",
    "school_count = school_count.withColumnRenamed( \"count(School Name)\", \"school_count\")\n",
    "school_count.show(1, vertical = True, truncate=100)\n",
    "school_count.createOrReplaceTempView(\"school\")\n",
    "\n",
    "# read ERP file and create tempview\n",
    "ERP_sdf = spark.read.csv(f\"../data/raw/ABS/ERP/ERP.csv\", header=True)\n",
    "ERP_sdf = ERP_sdf.filter(F.col(\"year\") == 2021)\n",
    "ERP_sdf.show(1, vertical = True, truncate=100)\n",
    "ERP_sdf.createOrReplaceTempView(\"ERP\")\n",
    "\n",
    "# read median household income file and create tempview\n",
    "household_sdf = spark.read.csv(f\"../data/raw/ABS/Household_income/Household_income.csv\", header=True)\n",
    "household_sdf.show(2, vertical = True, truncate=100)\n",
    "household_sdf.createOrReplaceTempView(\"household\")\n",
    "\n",
    "# read PTV station file and create tempview\n",
    "ptv_sdf = spark.read.csv(f\"../data/raw/PTV/public_trans.csv\", header=True)\n",
    "ptv_sdf.show(1, vertical = True, truncate=100)\n",
    "ptv_sdf.createOrReplaceTempView(\"ptv\")\n",
    "\n",
    "# read PTV station file and create tempview\n",
    "foi_Sdf = spark.read.csv(f\"../data/raw/FOI/foi_count_by_sa2.csv\", header=True)\n",
    "foi_Sdf.show(1, vertical = True, truncate=100)\n",
    "foi_Sdf.createOrReplaceTempView(\"foi\")\n",
    "\n",
    "# read median rent file and create tempview\n",
    "mrent_sdf = spark.read.csv(f\"../data/raw/DHHS/history_rent.csv\", header=True).dropna()\n",
    "mrent_sdf = mrent_sdf.filter((F.col(\"year\").cast(\"int\") == 2020) & (F.col(\"quarter\").cast(\"int\") == 4))\n",
    "mrent_sdf.show(1, vertical = True, truncate=100)\n",
    "mrent_sdf.createOrReplaceTempView(\"mrent\")\n",
    "\n",
    "# read distance to cbd and create tempview\n",
    "cbd_sdf = spark.read.csv(f\"../data/curated/sa2_to_cbd.csv\", header=True).dropna()\n",
    "cbd_sdf.show(1, vertical = True, truncate=100)\n",
    "cbd_sdf.createOrReplaceTempView(\"cbd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SA2', 'school_count']\n",
      "['SA2', 'year', 'population']\n",
      "['SA2', 'median_income']\n",
      "['SA2_CODE21', 'geometry', 'metrobus_count', 'metrotrain_count', 'metrotram_count', 'regbus_count', 'regcoach_count', 'regtrain_count', 'skybus_count']\n",
      "['SA2', 'recr_count', 'comm_count']\n",
      "['index', 'SA2', 'year', 'quarter', 'count', 'median']\n",
      "-RECORD 0---------------------\n",
      " SA2              | 202011018 \n",
      " school_count     | 13        \n",
      " ERP_population   | 14951     \n",
      " median_income    | 1267      \n",
      " metrobus_count   | 0         \n",
      " metrotrain_count | 0         \n",
      " metrotram_count  | 0         \n",
      " regbus_count     | 142       \n",
      " regcoach_count   | 2         \n",
      " regtrain_count   | 1         \n",
      " skybus_count     | 0         \n",
      " recr_count       | 1         \n",
      " comm_count       | 1         \n",
      " deal_count       | 709       \n",
      " median_rent      | 350.0     \n",
      " cbd_dis          | 152998.1  \n",
      "only showing top 1 row\n",
      "\n",
      "['SA2', 'school_count', 'ERP_population', 'median_income', 'metrobus_count', 'metrotrain_count', 'metrotram_count', 'regbus_count', 'regcoach_count', 'regtrain_count', 'skybus_count', 'recr_count', 'comm_count', 'deal_count', 'median_rent', 'cbd_dis']\n",
      "376\n"
     ]
    }
   ],
   "source": [
    "# inner join\n",
    "print(school_count.columns)\n",
    "print(ERP_sdf.columns)\n",
    "print(household_sdf.columns)\n",
    "print(ptv_sdf.columns)\n",
    "print(foi_Sdf.columns)\n",
    "print(mrent_sdf.columns)\n",
    "combine_sdf = spark.sql(\"\"\"\n",
    "    SELECT  school.SA2, school.school_count, \n",
    "        ERP.population AS ERP_population, median_income, \n",
    "        metrobus_count, metrotrain_count, metrotram_count, \n",
    "        regbus_count, regcoach_count, regtrain_count, skybus_count,\n",
    "        recr_count, comm_count, mrent.count AS deal_count,\n",
    "        mrent.median AS median_rent, cbd_dis\n",
    "    FROM school\n",
    "    INNER JOIN ERP ON school.SA2 = ERP.SA2\n",
    "    INNER JOIN household ON school.SA2 = household.SA2\n",
    "    INNER JOIN ptv ON school.SA2 = ptv.SA2_CODE21\n",
    "    INNER JOIN foi ON school.SA2 = foi.SA2\n",
    "    INNER JOIN mrent ON school.SA2 = mrent.SA2\n",
    "    INNER JOIN cbd ON school.SA2 = cbd.SA2\n",
    "\"\"\")\n",
    "combine_sdf.show(1, vertical = True, truncate=100)\n",
    "print(combine_sdf.columns)\n",
    "print(combine_sdf.count())\n",
    "combine_sdf.toPandas().to_csv(\n",
    "    f\"../data/curated/sa2_info.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## history_info\n",
    "\n",
    "#### ERP + price_index + interest_rate + median_rent + exchange_rate + immigration + debt_income_ratio -> BY year and SA2\n",
    "\n",
    "#### Read and prepare all data sets and create TempView\n",
    "\n",
    "fill data with mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------\n",
      " year       | 2009 \n",
      " debt_ratio | 0.97 \n",
      " quarter    | 4    \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## read and reformate data\n",
    "ratio_sdf = spark.read.csv(f\"../data/raw/ABS/debt_income_ratio/debt_income_ratio.csv\", header=True)\n",
    "ratio_df = ratio_sdf.toPandas()[['year', 'debt_ratio']]\n",
    "ratio_df[\"year\"] = ratio_df[\"year\"].astype(int)\n",
    "ratio_df[\"debt_ratio\"] = ratio_df[\"debt_ratio\"].astype(float)\n",
    "ratio_df = ratio_df.sort_values(by=[\"year\"]).reset_index(drop=True)\n",
    "ratio_df[\"quarter\"] = 4\n",
    "\n",
    "## Add second quarter data by taking average\n",
    "num_row = ratio_df.shape[0]\n",
    "new_df = pd.DataFrame(columns = ['year', 'debt_ratio', 'quarter'])\n",
    "for i in range(num_row-1):\n",
    "    cur_data = list(ratio_df.loc[i].values.flatten())\n",
    "    new_data = list(ratio_df.loc[i+1].values.flatten())\n",
    "    new_data[2] = 2\n",
    "    new_data[1] = (cur_data[1] + new_data[1])/2\n",
    "    new_df.loc[i] = new_data\n",
    "\n",
    "ratio_df = pd.concat([ratio_df, new_df])\n",
    "ratio_df = ratio_df.sort_values(by=['year', 'quarter']).reset_index(drop=True)\n",
    "\n",
    "## Add first and third quarter data by taking average\n",
    "num_row = ratio_df.shape[0]\n",
    "new_df = pd.DataFrame(columns = ['year', 'debt_ratio', 'quarter'])\n",
    "for i in range(num_row-1):\n",
    "    cur_data = list(ratio_df.loc[i].values.flatten())\n",
    "    new_data = list(ratio_df.loc[i+1].values.flatten())\n",
    "    if new_data[2] == 2:\n",
    "        new_data[2] = 1\n",
    "    else:\n",
    "        new_data[2] = 3\n",
    "    new_data[1] = (cur_data[1] + new_data[1])/2\n",
    "    new_df.loc[i] = new_data\n",
    "\n",
    "ratio_df = pd.concat([ratio_df, new_df])\n",
    "ratio_df = ratio_df.sort_values(by=['year', 'quarter']).reset_index(drop=True)\n",
    "\n",
    "## create tempiew\n",
    "# print(ratio_df[\"year\"].unique())\n",
    "ratio_df[\"year\"] = ratio_df[\"year\"].astype(int)\n",
    "ratio_df[\"quarter\"] = ratio_df[\"quarter\"].astype(int)\n",
    "ratio_df[\"debt_ratio\"] = ratio_df[\"debt_ratio\"].astype(float)\n",
    "ratio_sdf = spark.createDataFrame(ratio_df)\n",
    "ratio_sdf.show(1, vertical = True, truncate=100)\n",
    "ratio_sdf.createOrReplaceTempView(\"ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------\n",
      " year       | 2004  \n",
      " immi_count | 81230 \n",
      " quarter    | 4     \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_sdf = spark.read.csv(f\"../data/raw/ABS/immigration/immigration.csv\", header=True)\n",
    "immi_df = immigration_sdf.toPandas()[['year', 'immi_count']]\n",
    "immi_df = immi_df.astype(int)\n",
    "immi_df = immi_df.sort_values(by=[\"year\"]).reset_index(drop=True)\n",
    "immi_df[\"quarter\"] = 4\n",
    "\n",
    "## Add second quarter data by taking average\n",
    "num_row = immi_df.shape[0]\n",
    "new_df = pd.DataFrame(columns = ['year', 'immi_count', 'quarter'])\n",
    "for i in range(num_row-1):\n",
    "    cur_data = list(immi_df.loc[i].values.flatten())\n",
    "    new_data = list(immi_df.loc[i+1].values.flatten())\n",
    "    new_data[2] = 2\n",
    "    new_data[1] = int((cur_data[1] + new_data[1])/2)\n",
    "    new_df.loc[i] = new_data\n",
    "\n",
    "immi_df = pd.concat([immi_df, new_df])\n",
    "immi_df = immi_df.sort_values(by=['year', 'quarter']).reset_index(drop=True)\n",
    "\n",
    "## Add first and third quarter data by taking average\n",
    "num_row = immi_df.shape[0]\n",
    "new_df = pd.DataFrame(columns = ['year', 'immi_count', 'quarter'])\n",
    "for i in range(num_row-1):\n",
    "    cur_data = list(immi_df.loc[i].values.flatten())\n",
    "    new_data = list(immi_df.loc[i+1].values.flatten())\n",
    "    if new_data[2] == 2:\n",
    "        new_data[2] = 1\n",
    "    else:\n",
    "        new_data[2] = 3\n",
    "    new_data[1] = int((cur_data[1] + new_data[1])/2)\n",
    "    new_df.loc[i] = new_data\n",
    "\n",
    "immi_df = pd.concat([immi_df, new_df])\n",
    "immi_df = immi_df.sort_values(by=['year', 'quarter']).reset_index(drop=True)\n",
    "\n",
    "## create tempiew\n",
    "# print(immi_df[\"year\"].unique())\n",
    "immigration_sdf = spark.createDataFrame(immi_df)\n",
    "immigration_sdf.createOrReplaceTempView(\"immigration\")\n",
    "immigration_sdf.show(1, vertical = True, truncate=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------\n",
      " SA2        | 201011001 \n",
      " year       | 2010      \n",
      " population | 7894      \n",
      " quarter    | 4         \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ERP_sdf = spark.read.csv(f\"../data/raw/ABS/ERP/ERP.csv\", header=True)\n",
    "ERP_df = ERP_sdf.toPandas()[['SA2', 'year', 'population']]\n",
    "ERP_df = ERP_df.astype(int)\n",
    "ERP_df = ERP_df.sort_values(by=[\"SA2\", \"year\"]).reset_index(drop=True)\n",
    "ERP_df[\"quarter\"] = 4\n",
    "\n",
    "## Add second quarter data by taking average\n",
    "num_row = ERP_df.shape[0]\n",
    "new_df = pd.DataFrame(columns = ['SA2', 'year', 'population', 'quarter'])\n",
    "for i in range(num_row-1):\n",
    "    cur_data = list(ERP_df.loc[i].values.flatten())\n",
    "    new_data = list(ERP_df.loc[i+1].values.flatten())\n",
    "    if(cur_data[0] == new_data[0]):\n",
    "        new_data[3] = 2\n",
    "        new_data[2] = int((cur_data[2] + new_data[2])/2)\n",
    "        new_df.loc[i] = new_data\n",
    "\n",
    "ERP_df = pd.concat([ERP_df, new_df])\n",
    "ERP_df = ERP_df.sort_values(by=[\"SA2\", \"year\", \"quarter\"]).reset_index(drop=True)\n",
    "\n",
    "## Add first and third quarter data by taking average\n",
    "num_row = ERP_df.shape[0]\n",
    "new_df = pd.DataFrame(columns = ['SA2', 'year', 'population', 'quarter'])\n",
    "for i in range(num_row-1):\n",
    "    cur_data = list(ERP_df.loc[i].values.flatten())\n",
    "    new_data = list(ERP_df.loc[i+1].values.flatten())\n",
    "    if(cur_data[0] == new_data[0]):\n",
    "        if new_data[3] == 2:\n",
    "            new_data[3] = 1\n",
    "        else:\n",
    "            new_data[3] = 3\n",
    "        new_data[2] = int((cur_data[2] + new_data[2])/2)\n",
    "        new_df.loc[i] = new_data\n",
    "\n",
    "ERP_df = pd.concat([ERP_df, new_df])\n",
    "ERP_df = ERP_df.sort_values(by=[\"SA2\", \"year\", \"quarter\"]).reset_index(drop=True)\n",
    "\n",
    "## create tempiew\n",
    "# print(ERP_df[\"year\"].unique())\n",
    "ERP_sdf = spark.createDataFrame(ERP_df)\n",
    "ERP_sdf.createOrReplaceTempView(\"ERP\")\n",
    "ERP_sdf.show(1, vertical = True, truncate=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read other data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------\n",
      " year    | 2013 \n",
      " quarter | 3    \n",
      " bond    | 2.9  \n",
      "only showing top 1 row\n",
      "\n",
      "-RECORD 0------------\n",
      " year        | 2011  \n",
      " quarter     | 1     \n",
      " price_index | 104.0 \n",
      "only showing top 1 row\n",
      "\n",
      "-RECORD 0------------\n",
      " SA2     | 201011001 \n",
      " year    | 1999      \n",
      " quarter | 2         \n",
      " count   | 687       \n",
      " median  | 136.5     \n",
      "only showing top 1 row\n",
      "\n",
      "-RECORD 0---------\n",
      " year    | 2010   \n",
      " quarter | 1      \n",
      " to_USD  | 0.9159 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "month_l = [3, 6, 9, 12]\n",
    "\n",
    "# read population projection file and create tempview\n",
    "interest_sdf = spark.read.csv(f\"../data/raw/rba/interest_rate/interest_rate.csv\", header=True)\n",
    "interest_sdf = interest_sdf[[\"year\", \"quarter\", \"month\", \"bond\"]]\n",
    "interest_sdf = interest_sdf.withColumn(\"year\", F.col(\"year\").cast(\"int\"))\\\n",
    "    .withColumn(\"quarter\", F.col(\"quarter\").cast(\"int\"))\\\n",
    "    .withColumn(\"month\", F.col(\"month\").cast(\"int\"))\\\n",
    "    .withColumn(\"bond\", F.col(\"bond\").cast(\"double\"))\n",
    "interest_sdf = interest_sdf.filter(F.col(\"month\").isin(month_l))\n",
    "interest_sdf = interest_sdf[[\"year\", \"quarter\", \"bond\"]]\n",
    "interest_sdf.show(1, vertical = True, truncate=100)\n",
    "interest_sdf.createOrReplaceTempView(\"interest\")\n",
    "\n",
    "# read property price index file and create tempview\n",
    "index_sdf = spark.read.csv(f\"../data/raw/ABS/Price_index/Price_index.csv\", header=True)\n",
    "index_sdf = index_sdf[[\"year\", \"quarter\", \"price_index\"]]\n",
    "index_sdf = index_sdf.withColumn(\"year\", F.col(\"year\").cast(\"int\"))\\\n",
    "    .withColumn(\"quarter\", F.col(\"quarter\").cast(\"int\"))\\\n",
    "    .withColumn(\"price_index\", F.col(\"price_index\").cast(\"double\"))\n",
    "index_sdf.show(1, vertical = True, truncate=100)\n",
    "index_sdf.createOrReplaceTempView(\"index\")\n",
    "\n",
    "# read median property rent file and create tempview\n",
    "mrent_sdf = spark.read.csv(f\"../data/raw/DHHS/history_rent.csv\", header=True)\n",
    "mrent_sdf = mrent_sdf[[\"SA2\", \"year\", \"quarter\", \"count\", \"median\"]]\n",
    "mrent_sdf = mrent_sdf.withColumn(\"year\", F.col(\"year\").cast(\"int\"))\\\n",
    "    .withColumn(\"quarter\", F.col(\"quarter\").cast(\"int\"))\\\n",
    "    .withColumn(\"SA2\", F.col(\"SA2\").cast(\"int\"))\\\n",
    "    .withColumn(\"count\", F.col(\"count\").cast(\"int\"))\\\n",
    "    .withColumn(\"median\", F.col(\"median\").cast(\"double\"))\n",
    "mrent_sdf.show(1, vertical = True, truncate=100)\n",
    "mrent_sdf.createOrReplaceTempView(\"mrent\")\n",
    "\n",
    "# read exchange rate file and create tempview\n",
    "exchange_sdf = spark.read.csv(f\"../data/raw/rba/exchange_rate/exchange_rate.csv\", header=True)\n",
    "exchange_sdf = exchange_sdf[[\"year\", \"quarter\", \"month\",  \"to_USD\"]]\n",
    "exchange_sdf = exchange_sdf.withColumn(\"year\", F.col(\"year\").cast(\"int\"))\\\n",
    "    .withColumn(\"quarter\", F.col(\"quarter\").cast(\"int\"))\\\n",
    "    .withColumn(\"month\", F.col(\"month\").cast(\"int\"))\\\n",
    "    .withColumn(\"to_USD\", F.col(\"to_USD\").cast(\"double\"))\n",
    "exchange_sdf = exchange_sdf.filter(F.col(\"month\").isin(month_l))\n",
    "exchange_sdf = exchange_sdf[[\"year\", \"quarter\", \"to_USD\"]]\n",
    "exchange_sdf.show(1, vertical = True, truncate=100)\n",
    "exchange_sdf.createOrReplaceTempView(\"exchange\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SA2', 'year', 'population', 'quarter']\n",
      "['year', 'quarter', 'bond']\n",
      "['year', 'quarter', 'price_index']\n",
      "['SA2', 'year', 'quarter', 'count', 'median']\n",
      "['year', 'quarter', 'to_USD']\n",
      "['year', 'immi_count', 'quarter']\n",
      "['year', 'debt_ratio', 'quarter']\n",
      "-RECORD 0-------------------------\n",
      " SA2         | 201011001          \n",
      " year        | 2013               \n",
      " quarter     | 3                  \n",
      " population  | 9550               \n",
      " bond        | 2.9                \n",
      " price_index | 105.0              \n",
      " deal_count  | 1027               \n",
      " median_rent | 280.0              \n",
      " to_USD      | 0.9309             \n",
      " immi_count  | 121500             \n",
      " debt_ratio  | 0.8587499999999999 \n",
      "only showing top 1 row\n",
      "\n",
      "['SA2', 'year', 'quarter', 'population', 'bond', 'price_index', 'deal_count', 'median_rent', 'to_USD', 'immi_count', 'debt_ratio']\n",
      "501\n",
      "[Row(year=2014), Row(year=2016), Row(year=2018), Row(year=2017), Row(year=2013), Row(year=2019), Row(year=2015)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# inner join\n",
    "print(ERP_sdf.columns)\n",
    "print(interest_sdf.columns)\n",
    "print(index_sdf.columns)\n",
    "print(mrent_sdf.columns)\n",
    "print(exchange_sdf.columns)\n",
    "print(immigration_sdf.columns)\n",
    "print(ratio_sdf.columns)\n",
    "combine_sdf = spark.sql(\"\"\"\n",
    "    SELECT ERP.SA2, ERP.year, ERP.quarter, population, bond, price_index,\n",
    "        count AS deal_count, median AS median_rent, to_USD, immi_count, debt_ratio\n",
    "    FROM ERP\n",
    "    INNER JOIN interest ON ((ERP.year = interest.year)\n",
    "        AND (ERP.quarter = interest.quarter))\n",
    "    INNER JOIN index ON (ERP.year = index.year) \n",
    "        AND (ERP.quarter = index.quarter)\n",
    "    INNER JOIN mrent ON (ERP.year = mrent.year) \n",
    "        AND (ERP.quarter = mrent.quarter)\n",
    "        AND (ERP.SA2 = mrent.SA2)\n",
    "    INNER JOIN exchange ON (ERP.year = exchange.year) \n",
    "        AND (ERP.quarter = exchange.quarter)\n",
    "    INNER JOIN immigration ON (ERP.year = immigration.year) \n",
    "        AND (ERP.quarter = immigration.quarter)\n",
    "    INNER JOIN ratio ON (ERP.year = ratio.year) \n",
    "        AND (ERP.quarter = ratio.quarter)\n",
    "\"\"\")\n",
    "\n",
    "combine_sdf = combine_sdf.sort(\"SA2\", \"year\", \"quarter\")\n",
    "combine_sdf.show(1, vertical = True, truncate=100)\n",
    "print(combine_sdf.columns)\n",
    "print(len(combine_sdf.select(\"SA2\").distinct().collect()))\n",
    "print(combine_sdf.select(\"year\").distinct().collect())\n",
    "combine_sdf.toPandas().to_csv(\n",
    "    f\"../data/curated/history_info.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-------+----------+----+-----------+----------+-----------+------+----------+------------------+\n",
      "|      SA2|year|quarter|population|bond|price_index|deal_count|median_rent|to_USD|immi_count|        debt_ratio|\n",
      "+---------+----+-------+----------+----+-----------+----------+-----------+------+----------+------------------+\n",
      "|201011481|2013|      3|      8981| 2.9|      105.0|      1140|      275.0|0.9309|    121500|0.8587499999999999|\n",
      "|201011481|2013|      4|      9008|2.96|      109.0|      1146|      275.0|0.8948|    122250|              0.86|\n",
      "|201011481|2014|      1|      9024|2.97|      110.0|      1344|      280.0|0.9221|    123730|0.8587499999999999|\n",
      "|201011481|2014|      2|      9041| 2.8|      112.0|      1181|      280.0| 0.942|    125210|0.8574999999999999|\n",
      "|201011481|2014|      3|      9057| 2.8|      113.0|      1153|      285.0|0.8752|    126690|           0.85625|\n",
      "|201011481|2014|      4|      9074|2.28|      115.0|      1186|      285.0|0.8202|    128170|             0.855|\n",
      "|201011481|2015|      1|      9102|1.83|      115.0|      1303|      284.0|0.7634|    131182|           0.85375|\n",
      "|201011481|2015|      2|      9130|2.03|      120.0|      1201|      280.0| 0.768|    134195|            0.8525|\n",
      "|201011481|2015|      3|      9158|1.86|      124.0|      1251|      284.0| 0.701|    137207|0.8512500000000001|\n",
      "|201011481|2015|      4|      9186| 2.1|      126.0|      1263|      290.0|0.7306|    140220|              0.85|\n",
      "|201011481|2016|      1|      9219|1.95|      127.0|      1350|      290.0|0.7657|    145072|0.8912499999999999|\n",
      "|201011481|2016|      2|      9253|1.58|      130.0|      1264|      287.5|0.7426|    149925|0.9324999999999999|\n",
      "|201011481|2016|      3|      9287|1.56|      132.0|      1275|      290.0| 0.763|    154777|0.9737499999999999|\n",
      "|201011481|2016|      4|      9321|1.97|      140.0|      1165|      300.0|0.7236|    159630|             1.015|\n",
      "|201011481|2017|      1|      9356|1.99|      144.0|      1268|      295.0|0.7644|    160015|           1.05625|\n",
      "|201011481|2017|      2|      9391|1.76|      148.0|      1181|      300.0|0.7692|    160400|            1.0975|\n",
      "|201011481|2017|      3|      9426|2.06|      150.0|      1139|      307.5|0.7839|    160785|           1.13875|\n",
      "|201011481|2017|      4|      9462|2.03|      154.0|      1154|      310.0|  0.78|    161170|              1.18|\n",
      "|201011481|2018|      1|      9479| 2.1|      153.0|      1316|      320.0|0.7665|    162955|1.1712500000000001|\n",
      "|201011481|2018|      2|      9496|2.13|      152.0|      1037|      320.0|0.7391|    164740|            1.1625|\n",
      "|201011481|2018|      3|      9513|2.06|      148.0|      1088|      325.0|0.7222|    166525|           1.15375|\n",
      "|201011481|2018|      4|      9530|1.93|      144.0|      1097|      330.0|0.7058|    168310|             1.145|\n",
      "|201011481|2019|      1|      9553|1.53|      139.0|      1221|      330.0|0.7087|    163890|           1.13625|\n",
      "|201011481|2019|      2|      9576|0.99|      137.0|      1072|      335.0|0.7013|    159470|            1.1275|\n",
      "|201011481|2019|      3|      9599|0.78|      142.0|      1139|      340.0|0.6749|    155050|           1.11875|\n",
      "|201011481|2019|      4|      9623|0.77|      150.0|      1158|      340.0|0.7006|    150630|              1.11|\n",
      "+---------+----+-------+----------+----+-----------+----------+-----------+------+----------+------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(combine_sdf.filter(F.col(\"SA2\").cast(\"int\") == 201011481).show(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rent_info\n",
    "- SA2(ERP + household + school_count + PTV + FOI) + rent(basic + distance) -> rent_price\n",
    "- ../data/curated/rent_info.csv\n",
    "- rent (float), bedroom (int), baths (int), parking (int), school_dis (float), station_dis (float), SA2 (int), school_count (int), ERP_population (int), median_income (float), metrobus_count (int), metrotrain_count (int), metrotram_count (int), regbus_count (int), regcoach_count (int), regtrain_count (int), skybus_count (int), recr_count (int), comm_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------\n",
      " _c0              | 0         \n",
      " SA2              | 202011018 \n",
      " school_count     | 13        \n",
      " ERP_population   | 14951     \n",
      " median_income    | 1267      \n",
      " metrobus_count   | 0         \n",
      " metrotrain_count | 0         \n",
      " metrotram_count  | 0         \n",
      " regbus_count     | 142       \n",
      " regcoach_count   | 2         \n",
      " regtrain_count   | 1         \n",
      " skybus_count     | 0         \n",
      " recr_count       | 1         \n",
      " comm_count       | 1         \n",
      " deal_count       | 709       \n",
      " median_rent      | 350.0     \n",
      " cbd_dis          | 152998.1  \n",
      "only showing top 1 row\n",
      "\n",
      "-RECORD 0------------------\n",
      " _c0         | 0           \n",
      " SA2         | 201011001   \n",
      " rent        | 490.0       \n",
      " bedroom     | 4           \n",
      " baths       | 2           \n",
      " parking     | 2           \n",
      " Latitude    | -37.5630731 \n",
      " Longitude   | 143.7938749 \n",
      " school_dis  | 1651.7      \n",
      " station_dis | 5895.5      \n",
      " cbd_dis     | 350.0       \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read SA2 file and create tempview\n",
    "SA2_sdf = spark.read.csv(f\"../data/curated/sa2_info.csv\", header=True)\n",
    "SA2_sdf.show(1, vertical = True, truncate=100)\n",
    "SA2_sdf.createOrReplaceTempView(\"SA2\")\n",
    "\n",
    "# read rent data file and create tempview\n",
    "rent_sdf = spark.read.csv(f\"../data/curated/rent_distance.csv\", header=True)\n",
    "rent_sdf.show(1, vertical = True, truncate=100)\n",
    "rent_sdf.createOrReplaceTempView(\"rent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_c0', 'SA2', 'school_count', 'ERP_population', 'median_income', 'metrobus_count', 'metrotrain_count', 'metrotram_count', 'regbus_count', 'regcoach_count', 'regtrain_count', 'skybus_count', 'recr_count', 'comm_count', 'deal_count', 'median_rent', 'cbd_dis']\n",
      "['_c0', 'SA2', 'rent', 'bedroom', 'baths', 'parking', 'Latitude', 'Longitude', 'school_dis', 'station_dis', 'cbd_dis']\n",
      "-RECORD 0---------------------\n",
      " rent             | 490.0     \n",
      " bedroom          | 4         \n",
      " baths            | 2         \n",
      " parking          | 2         \n",
      " school_dis       | 1651.7    \n",
      " station_dis      | 5895.5    \n",
      " cbd_dis          | 350.0     \n",
      " median_rent      | 367.5     \n",
      " SA2              | 201011001 \n",
      " school_count     | 4         \n",
      " ERP_population   | 16823     \n",
      " median_income    | 1952      \n",
      " metrobus_count   | 0         \n",
      " metrotrain_count | 0         \n",
      " metrotram_count  | 0         \n",
      " regbus_count     | 43        \n",
      " regcoach_count   | 0         \n",
      " regtrain_count   | 0         \n",
      " skybus_count     | 0         \n",
      " recr_count       | 0         \n",
      " comm_count       | 0         \n",
      "only showing top 1 row\n",
      "\n",
      "['rent', 'bedroom', 'baths', 'parking', 'school_dis', 'station_dis', 'cbd_dis', 'median_rent', 'SA2', 'school_count', 'ERP_population', 'median_income', 'metrobus_count', 'metrotrain_count', 'metrotram_count', 'regbus_count', 'regcoach_count', 'regtrain_count', 'skybus_count', 'recr_count', 'comm_count']\n"
     ]
    }
   ],
   "source": [
    "# inner join\n",
    "print(SA2_sdf.columns)\n",
    "print(rent_sdf.columns)\n",
    "combine_sdf = spark.sql(\"\"\"\n",
    "    SELECT  rent, bedroom, baths, parking, school_dis, station_dis, \n",
    "    rent.cbd_dis, median_rent, rent.SA2, school_count, ERP_population, \n",
    "    median_income, metrobus_count, metrotrain_count, metrotram_count, \n",
    "    regbus_count, regcoach_count, regtrain_count, skybus_count, recr_count, comm_count\n",
    "    FROM SA2\n",
    "    INNER JOIN rent ON SA2.SA2 = rent.SA2\n",
    "\"\"\")\n",
    "combine_sdf.show(1, vertical = True, truncate=100)\n",
    "print(combine_sdf.columns)\n",
    "combine_sdf.toPandas().to_csv(\n",
    "    f\"../data/curated/rent_info.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Junhua Liu for study use only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
