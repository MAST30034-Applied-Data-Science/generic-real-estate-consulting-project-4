{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERP + household + school_count + PTV + median_rent + cbd_dis-> by sa2\n",
    "##### INPUT:\n",
    "School location\n",
    "- ../data/raw/ACARA/School_location/School_location.csv\n",
    "- School Name (String), SA2 (int), Latitude (float), Longitude (float), School Type (String)\n",
    "\n",
    "Estimated Resident Population (ERP) (2001 to 2021) (By SA2)\n",
    "- ../data/raw/ABS/ERP/ERP.csv\n",
    "- SA2 (int), year (int), population (int)\n",
    "\n",
    "Median household income (weekly) (2021) (By SA2)\n",
    "- ../data/raw/ABS/Household_income/Household_income.csv\n",
    "- SA2 (int), year (int),  household_type (String), income_level (String), popultaion (int)\n",
    "\n",
    "School location\n",
    "- ../data/raw/ACARA/School_location/School_location.csv\n",
    "- School Name (String), SA2 (int), Latitude (float), Longitude (float), School Type (String)\n",
    "\n",
    "Median rent\n",
    "- ../data/raw/DHHS/history_rent.csv\n",
    "- SA2 (int), year (int), month (int), count (int), median (float)\n",
    "\n",
    "distance to cbd\n",
    "- ../data/curated/sa2_to_cbd.csv\n",
    "- SA2 (int), cbd_dis (float)\n",
    "##### OUTPUT:\n",
    "SA2_info\n",
    "- ../data/curated/sa2_info.csv\n",
    "- SA2 (int), school_count (int), ERP_population (int), median_income (float), metrobus_count (int), metrotrain_count (int), metrotram_count (int), regbus_count (int), regcoach_count (int), regtrain_count (int), skybus_count (int), recr_count (int), comm_count (int), cbd_dis(float)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ERP + price_index + interest_rate + median_rent + exchange_rate + immigration + debt_income_ratio -> BY year and SA2\n",
    "##### INPUT:\n",
    "ERP\n",
    "- ../data/raw/ABS/ERP/ERP.csv\n",
    "- SA2 (int), year (int), population (int)\n",
    "\n",
    "Residential Property Price Index (1/2011 - 12/2021) (AUS)\n",
    "- ../data/raw/ABS/Price_index/Price_index.csv\n",
    "- year (int), quarter (int), price_index (int)\n",
    "\n",
    "Interest rate (2013 - 2021) (AUS)\n",
    "- ../data/raw/rba/interest_rate/interest_rate.csv\n",
    "- year(int), quarter(int), month(int), bond (float, risk free interest rate)\n",
    "\n",
    "Median rent (1999 - 2021) BY sa2\n",
    "- ../data/raw/DHHS/history_rent.csv\n",
    "- SA2 (int), year (int), quarter (int), count (int), median (float)\n",
    "\n",
    "Exchange Rate (2010 March - 2022 June) AUS\n",
    "- ../data/raw/rba/exchange_rate/exchange_rate.csv\n",
    "- year (int), quarter (int), month (int), to_USD (float)\n",
    "\n",
    "Immigration data, 2004 - 2019, Victoria only\n",
    "- ../data/raw/ABS/immigration/immigration.csv\n",
    "- year (int), immi_count (int)\n",
    "\n",
    "Household debt income ratio measured on each two years In Millions (2009-2019) (Victoria only)\n",
    "- ../data/raw/ABS/debt_income_ratio/debt_income_ratio.csv\n",
    "- year (int), debt_ratio (float)\n",
    "\n",
    "##### OUTPUT:\n",
    "history_info\n",
    "- ../data/curated/history_info.csv\n",
    "- SA2 (int), year (int), quarter (int), month (int), bond (float), price_index (int), population (int), median_rent (float), deal_count (int), to_USD (float), immi_count (int), debt_ratio (float)\n",
    "- ERP + price_index + interest_rate + median_rent + exchange_rate + immigration + debt_income_ratio\n",
    "- year: ERP, immigration, debt_ratio (2 years)\n",
    "- quarter: price_index, median_rent\n",
    "- month: interest_rate, exchange_rate\n",
    "\n",
    "\n",
    "# SA2(ERP + household + school_count + PTV + FOI + Median_rent + cbd_distance) + rent(basic + distance) -> rent_price\n",
    "##### INPUT:\n",
    "SA2:\n",
    "- ALL: ../data/curated/sa2_info.csv\n",
    "\n",
    "rent information with distance\n",
    "- ../data/curated/rent_distance.csv\n",
    "- rent_index (int), SA2 (int), rent (float), bedroom (int), baths (int), parking (int), Latitude (float), Longitude (float), school_dis (float), station_dis (float)\n",
    "\n",
    "##### OUTPUT:\n",
    "- ../data/curated/rent_info.csv\n",
    "- rent (float), bedroom (int), baths (int), parking (int), school_dis (float), station_dis (float), cbd_dis (float), median_rent (float), SA2 (int), school_count (int), ERP_population (int), median_income (float), metrobus_count (int), metrotrain_count (int), metrotram_count (int), regbus_count (int), regcoach_count (int), regtrain_count (int), skybus_count (int), recr_count (int), comm_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/25 23:01:31 WARN Utils: Your hostname, Bruce-PC resolves to a loopback address: 127.0.1.1; using 172.23.187.80 instead (on interface eth0)\n",
      "22/09/25 23:01:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/25 23:01:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/09/25 23:01:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import re\n",
    "from itertools import compress\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import zipfile\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Assignment_2\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "headers = {\"accept\": \"text/csv\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpd_station_merge(poly_gdf, file_path, by_id_name = \"SA2_CODE21\",\\\n",
    "    station_id_name = \"STOP_ID\", method={\"STOP_ID\": \"count\"}):\n",
    "    \"\"\"\n",
    "        A function used to merge shape file in path: {file_path} to a \n",
    "        geopandas dataframe {poly_gdf} with POLYGON geometry. \n",
    "        poly_gdf: a geopandas.GeoDataFrame object contains POLYGON geometry\n",
    "        file_path: a String of file path to read target shape file\n",
    "        by_id_name: a String of id name to perform groupby option\n",
    "        station_id_name: a String of id name stated in the readed gdf\n",
    "        method: a Dict of operations to perform after groupby\n",
    "    \"\"\"\n",
    "\n",
    "    ### read station file\n",
    "    station_gdf = gpd.read_file(file_path)\n",
    "\n",
    "    # metro bus station feature selection\n",
    "    station_gdf = station_gdf[[station_id_name, \"geometry\"]]\n",
    "    \n",
    "\n",
    "\n",
    "    # merge tabels\n",
    "    join_gdf = gpd.sjoin(poly_gdf, station_gdf, how=\"left\")\n",
    "    join_gdf = join_gdf.groupby(by_id_name).agg(method)\n",
    "    \n",
    "    return join_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SA2_info\n",
    "#### ERP + household + school_count + PTV + median_rent-> by sa2\n",
    "Read and prepare all data sets and create TempView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------\n",
      " SA2          | 209031212 \n",
      " school_count | 9         \n",
      "only showing top 1 row\n",
      "\n",
      "-RECORD 0---------------\n",
      " SA2        | 201011481 \n",
      " year       | 2021      \n",
      " population | 9656      \n",
      "only showing top 1 row\n",
      "\n",
      "-RECORD 0------------------\n",
      " SA2           | 213051589 \n",
      " median_income | 1862      \n",
      "-RECORD 1------------------\n",
      " SA2           | 209041437 \n",
      " median_income | 1979      \n",
      "only showing top 2 rows\n",
      "\n",
      "-RECORD 0----------------------------------------------------------------------------------------------------------------\n",
      " SA2_CODE21       | 201011001                                                                                            \n",
      " geometry         | POLYGON ((143.78282104711133 -37.566657808073295, 143.75557764214773 -37.56346721632544, 143.7480... \n",
      " metrobus_count   | 0                                                                                                    \n",
      " metrotrain_count | 0                                                                                                    \n",
      " metrotram_count  | 0                                                                                                    \n",
      " regbus_count     | 43                                                                                                   \n",
      " regcoach_count   | 0                                                                                                    \n",
      " regtrain_count   | 0                                                                                                    \n",
      " skybus_count     | 0                                                                                                    \n",
      "only showing top 1 row\n",
      "\n",
      "-RECORD 0---------------\n",
      " SA2        | 201011001 \n",
      " recr_count | 0         \n",
      " comm_count | 0         \n",
      "only showing top 1 row\n",
      "\n",
      "22/09/25 23:02:11 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , SA2, year, quarter, count, median\n",
      " Schema: _c0, SA2, year, quarter, count, median\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/bruce/projects/ass2/data/raw/DHHS/history_rent.csv\n",
      "(0 rows)\n",
      "\n",
      "22/09/25 23:02:11 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , SA2, cbd_dis\n",
      " Schema: _c0, SA2, cbd_dis\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/bruce/projects/ass2/data/curated/sa2_to_cbd.csv\n",
      "-RECORD 0------------\n",
      " _c0     | 0         \n",
      " SA2     | 201011001 \n",
      " cbd_dis | 350.0     \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read school file\n",
    "school_sdf = spark.read.csv(f\"../data/raw/ACARA/School_location/School_location.csv\", header=True)\n",
    "\n",
    "# count school by SA2\n",
    "school_count = school_sdf.groupBy(\"SA2\").agg({\n",
    "    \"School Name\": \"count\"\n",
    "})\n",
    "school_count = school_count.withColumnRenamed( \"count(School Name)\", \"school_count\")\n",
    "school_count.show(1, vertical = True, truncate=100)\n",
    "school_count.createOrReplaceTempView(\"school\")\n",
    "\n",
    "# read ERP file and create tempview\n",
    "ERP_sdf = spark.read.csv(f\"../data/raw/ABS/ERP/ERP.csv\", header=True)\n",
    "ERP_sdf = ERP_sdf.filter(F.col(\"year\") == 2021)\n",
    "ERP_sdf.show(1, vertical = True, truncate=100)\n",
    "ERP_sdf.createOrReplaceTempView(\"ERP\")\n",
    "\n",
    "# read median household income file and create tempview\n",
    "household_sdf = spark.read.csv(f\"../data/raw/ABS/Household_income/Household_income.csv\", header=True)\n",
    "household_sdf.show(2, vertical = True, truncate=100)\n",
    "household_sdf.createOrReplaceTempView(\"household\")\n",
    "\n",
    "# read PTV station file and create tempview\n",
    "ptv_sdf = spark.read.csv(f\"../data/raw/PTV/public_trans.csv\", header=True)\n",
    "ptv_sdf.show(1, vertical = True, truncate=100)\n",
    "ptv_sdf.createOrReplaceTempView(\"ptv\")\n",
    "\n",
    "# read PTV station file and create tempview\n",
    "foi_Sdf = spark.read.csv(f\"../data/raw/FOI/foi_count_by_sa2.csv\", header=True)\n",
    "foi_Sdf.show(1, vertical = True, truncate=100)\n",
    "foi_Sdf.createOrReplaceTempView(\"foi\")\n",
    "\n",
    "# read median rent file and create tempview\n",
    "mrent_sdf = spark.read.csv(f\"../data/raw/DHHS/history_rent.csv\", header=True).dropna()\n",
    "mrent_sdf = mrent_sdf.filter((F.col(\"year\") == 2021) & (F.col(\"quarter\") == 3))\n",
    "mrent_sdf.show(1, vertical = True, truncate=100)\n",
    "mrent_sdf.createOrReplaceTempView(\"mrent\")\n",
    "\n",
    "# read distance to cbd and create tempview\n",
    "cbd_sdf = spark.read.csv(f\"../data/curated/sa2_to_cbd.csv\", header=True).dropna()\n",
    "cbd_sdf.show(1, vertical = True, truncate=100)\n",
    "cbd_sdf.createOrReplaceTempView(\"cbd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SA2', 'school_count']\n",
      "['SA2', 'year', 'population']\n",
      "['SA2', 'median_income']\n",
      "['SA2_CODE21', 'geometry', 'metrobus_count', 'metrotrain_count', 'metrotram_count', 'regbus_count', 'regcoach_count', 'regtrain_count', 'skybus_count']\n",
      "['SA2', 'recr_count', 'comm_count']\n",
      "['_c0', 'SA2', 'year', 'quarter', 'count', 'median']\n",
      "22/09/25 23:02:17 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , SA2, cbd_dis\n",
      " Schema: _c0, SA2, cbd_dis\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/bruce/projects/ass2/data/curated/sa2_to_cbd.csv\n",
      "22/09/25 23:02:17 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , SA2, year, quarter, count, median\n",
      " Schema: _c0, SA2, year, quarter, count, median\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/bruce/projects/ass2/data/raw/DHHS/history_rent.csv\n",
      "(0 rows)\n",
      "\n",
      "['SA2', 'school_count', 'ERP_population', 'median_income', 'metrobus_count', 'metrotrain_count', 'metrotram_count', 'regbus_count', 'regcoach_count', 'regtrain_count', 'skybus_count', 'recr_count', 'comm_count', 'deal_count', 'median_rent', 'cbd_dis']\n",
      "22/09/25 23:02:18 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , SA2, year, quarter, count, median\n",
      " Schema: _c0, SA2, year, quarter, count, median\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/bruce/projects/ass2/data/raw/DHHS/history_rent.csv\n",
      "22/09/25 23:02:18 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , SA2, cbd_dis\n",
      " Schema: _c0, SA2, cbd_dis\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/bruce/projects/ass2/data/curated/sa2_to_cbd.csv\n"
     ]
    }
   ],
   "source": [
    "# inner join\n",
    "print(school_count.columns)\n",
    "print(ERP_sdf.columns)\n",
    "print(household_sdf.columns)\n",
    "print(ptv_sdf.columns)\n",
    "print(foi_Sdf.columns)\n",
    "print(mrent_sdf.columns)\n",
    "combine_sdf = spark.sql(\"\"\"\n",
    "    SELECT  school.SA2, school.school_count, \n",
    "        ERP.population AS ERP_population, median_income, \n",
    "        metrobus_count, metrotrain_count, metrotram_count, \n",
    "        regbus_count, regcoach_count, regtrain_count, skybus_count,\n",
    "        recr_count, comm_count, mrent.count AS deal_count,\n",
    "        mrent.median AS median_rent, cbd_dis\n",
    "    FROM school\n",
    "    INNER JOIN ERP ON school.SA2 = ERP.SA2\n",
    "    INNER JOIN household ON school.SA2 = household.SA2\n",
    "    INNER JOIN ptv ON school.SA2 = ptv.SA2_CODE21\n",
    "    INNER JOIN foi ON school.SA2 = foi.SA2\n",
    "    INNER JOIN mrent ON school.SA2 = mrent.SA2\n",
    "    INNER JOIN cbd ON school.SA2 = cbd.SA2\n",
    "\"\"\")\n",
    "combine_sdf.show(1, vertical = True, truncate=100)\n",
    "print(combine_sdf.columns)\n",
    "combine_sdf.toPandas().to_csv(\n",
    "    f\"../data/curated/sa2_info.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## history_info\n",
    "\n",
    "#### ERP + interest_rate + price_index -> year, SA2\n",
    "\n",
    "Read and prepare all data sets and create TempView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------\n",
      " SA2        | 201011481 \n",
      " year       | 2021      \n",
      " population | 9656      \n",
      "only showing top 1 row\n",
      "\n",
      "22/09/25 23:26:03 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , year, quarter, month, bond\n",
      " Schema: _c0, year, quarter, month, bond\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/bruce/projects/ass2/data/raw/rba/interest_rate/interest_rate.csv\n",
      "-RECORD 0-------\n",
      " _c0     | 0    \n",
      " year    | 2013 \n",
      " quarter | 3    \n",
      " month   | 7    \n",
      " bond    | 2.69 \n",
      "only showing top 1 row\n",
      "\n",
      "-RECORD 0-----------\n",
      " year        | 2012 \n",
      " price_index | 100  \n",
      " quarter     | 4    \n",
      "only showing top 1 row\n",
      "\n",
      "22/09/25 23:26:03 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , SA2, year, quarter, count, median\n",
      " Schema: _c0, SA2, year, quarter, count, median\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/bruce/projects/ass2/data/raw/DHHS/history_rent.csv\n",
      "-RECORD 0----------------\n",
      " _c0         | 0         \n",
      " SA2         | 201011001 \n",
      " year        | 1999      \n",
      " quarter     | 2         \n",
      " deal_count  | 687       \n",
      " median_rent | 136.5     \n",
      "only showing top 1 row\n",
      "\n",
      "22/09/25 23:26:03 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , year, quarter, month, to_USD\n",
      " Schema: _c0, year, quarter, month, to_USD\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/bruce/projects/ass2/data/raw/rba/exchange_rate/exchange_rate.csv\n",
      "-RECORD 0---------\n",
      " _c0     | 0      \n",
      " year    | 2010   \n",
      " quarter | 1      \n",
      " month   | 3      \n",
      " to_USD  | 0.9159 \n",
      "only showing top 1 row\n",
      "\n",
      "22/09/25 23:26:03 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , year, immi_count\n",
      " Schema: _c0, year, immi_count\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/bruce/projects/ass2/data/raw/ABS/immigration/immigration.csv\n",
      "-RECORD 0-----------\n",
      " _c0        | 0     \n",
      " year       | 2004  \n",
      " immi_count | 81230 \n",
      "only showing top 1 row\n",
      "\n",
      "22/09/25 23:26:03 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , year, debt_ratio\n",
      " Schema: _c0, year, debt_ratio\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/bruce/projects/ass2/data/raw/ABS/debt_income_ratio/debt_income_ratio.csv\n",
      "-RECORD 0----------\n",
      " _c0        | 0    \n",
      " year       | 2009 \n",
      " debt_ratio | 0.97 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read ERP file and create tempview\n",
    "ERP_sdf = spark.read.csv(f\"../data/raw/ABS/ERP/ERP.csv\", header=True)\n",
    "ERP_sdf.show(1, vertical = True, truncate=100)\n",
    "ERP_sdf.createOrReplaceTempView(\"ERP\")\n",
    "\n",
    "# read population projection file and create tempview\n",
    "interest_sdf = spark.read.csv(f\"../data/raw/rba/interest_rate/interest_rate.csv\", header=True)\n",
    "interest_sdf.show(1, vertical = True, truncate=100)\n",
    "interest_sdf.createOrReplaceTempView(\"interest\")\n",
    "\n",
    "# read property price index file and create tempview\n",
    "index_sdf = spark.read.csv(f\"../data/raw/ABS/Price_index/Price_index.csv\", header=True)\n",
    "index_sdf.show(1, vertical = True, truncate=100)\n",
    "index_sdf.createOrReplaceTempView(\"index\")\n",
    "\n",
    "# read median property rent file and create tempview\n",
    "mrent_sdf = spark.read.csv(f\"../data/raw/DHHS/history_rent.csv\", header=True)\n",
    "mrent_sdf = mrent_sdf.withColumnRenamed( \"count\", \"deal_count\")\n",
    "mrent_sdf = mrent_sdf.withColumnRenamed( \"median\", \"median_rent\")\n",
    "mrent_sdf.show(1, vertical = True, truncate=100)\n",
    "mrent_sdf.createOrReplaceTempView(\"mrent\")\n",
    "\n",
    "# read exchange rate file and create tempview\n",
    "exchange_sdf = spark.read.csv(f\"../data/raw/rba/exchange_rate/exchange_rate.csv\", header=True)\n",
    "exchange_sdf.show(1, vertical = True, truncate=100)\n",
    "exchange_sdf.createOrReplaceTempView(\"exchange\")\n",
    "\n",
    "# read exchange rate file and create tempview\n",
    "immigration_sdf = spark.read.csv(f\"../data/raw/ABS/immigration/immigration.csv\", header=True)\n",
    "immigration_sdf.show(1, vertical = True, truncate=100)\n",
    "immigration_sdf.createOrReplaceTempView(\"immigration\")\n",
    "\n",
    "# read exchange rate file and create tempview\n",
    "ratio_sdf = spark.read.csv(f\"../data/raw/ABS/debt_income_ratio/debt_income_ratio.csv\", header=True)\n",
    "ratio_sdf.show(1, vertical = True, truncate=100)\n",
    "ratio_sdf.createOrReplaceTempView(\"ratio\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SA2', 'year', 'population']\n",
      "['_c0', 'year', 'quarter', 'month', 'bond']\n",
      "['year', 'price_index', 'quarter']\n",
      "['_c0', 'SA2', 'year', 'quarter', 'deal_count', 'median_rent']\n",
      "['_c0', 'year', 'quarter', 'month', 'to_USD']\n",
      "['_c0', 'year', 'immi_count']\n",
      "['_c0', 'year', 'debt_ratio']\n",
      "-RECORD 0----------------\n",
      " SA2         | 201011481 \n",
      " year        | 2019      \n",
      " quarter     | 4         \n",
      " month       | 12        \n",
      " bond        | 0.77      \n",
      " price_index | 150       \n",
      " population  | 9623      \n",
      " median_rent | 340.0     \n",
      " deal_count  | 1158      \n",
      " to_USD      | 0.7006    \n",
      " immi_count  | 150630    \n",
      " debt_ratio  | 1.11      \n",
      "only showing top 1 row\n",
      "\n",
      "['SA2', 'year', 'quarter', 'month', 'bond', 'price_index', 'population', 'median_rent', 'deal_count', 'to_USD', 'immi_count', 'debt_ratio']\n",
      "501\n",
      "[Row(year='2016'), Row(year='2019'), Row(year='2017'), Row(year='2014'), Row(year='2013'), Row(year='2018'), Row(year='2015')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# inner join\n",
    "print(ERP_sdf.columns)\n",
    "print(interest_sdf.columns)\n",
    "print(index_sdf.columns)\n",
    "print(mrent_sdf.columns)\n",
    "print(exchange_sdf.columns)\n",
    "print(immigration_sdf.columns)\n",
    "print(ratio_sdf.columns)\n",
    "combine_sdf = spark.sql(\"\"\"\n",
    "    SELECT base.SA2, base.year, base.quarter, base.month, bond, price_index,\n",
    "        population, median_rent, deal_count, to_USD, immi_count, debt_ratio\n",
    "    FROM \n",
    "        (SELECT ERP.SA2, interest.year, interest.quarter, interest.month,\n",
    "            bond, population\n",
    "        FROM interest\n",
    "        INNER JOIN ERP ON interest.year = ERP.year) AS base\n",
    "    INNER JOIN index ON (base.year = index.year) \n",
    "        AND (base.quarter = index.quarter)\n",
    "    INNER JOIN mrent ON (base.year = mrent.year) \n",
    "        AND (base.SA2 = mrent.SA2)\n",
    "    INNER JOIN exchange ON (base.year = exchange.year) \n",
    "        AND (base.quarter = exchange.quarter) \n",
    "        AND (base.month = exchange.month)\n",
    "    INNER JOIN immigration ON base.year = immigration.year\n",
    "    INNER JOIN ratio ON base.year = ratio.year\n",
    "\"\"\")\n",
    "combine_sdf.show(1, vertical = True, truncate=100)\n",
    "print(combine_sdf.columns)\n",
    "print(len(combine_sdf.select(\"SA2\").distinct().collect()))\n",
    "print(combine_sdf.select(\"year\").distinct().collect())\n",
    "combine_sdf.toPandas().to_csv(\n",
    "    f\"../data/curated/history_info.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rent_info\n",
    "- SA2(ERP + household + school_count + PTV + FOI) + rent(basic + distance) -> rent_price\n",
    "- ../data/curated/rent_info.csv\n",
    "- rent (float), bedroom (int), baths (int), parking (int), school_dis (float), station_dis (float), SA2 (int), school_count (int), ERP_population (int), median_income (float), metrobus_count (int), metrotrain_count (int), metrotram_count (int), regbus_count (int), regcoach_count (int), regtrain_count (int), skybus_count (int), recr_count (int), comm_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/20 16:59:17 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , SA2, school_count, ERP_population, median_income, metrobus_count, metrotrain_count, metrotram_count, regbus_count, regcoach_count, regtrain_count, skybus_count, recr_count, comm_count, deal_count, median_rent, cbd_dis\n",
      " Schema: _c0, SA2, school_count, ERP_population, median_income, metrobus_count, metrotrain_count, metrotram_count, regbus_count, regcoach_count, regtrain_count, skybus_count, recr_count, comm_count, deal_count, median_rent, cbd_dis\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/bruce/projects/ass2/data/curated/sa2_info.csv\n",
      "-RECORD 0---------------------\n",
      " _c0              | 0         \n",
      " SA2              | 202011018 \n",
      " school_count     | 13        \n",
      " ERP_population   | 14951     \n",
      " median_income    | 1267      \n",
      " metrobus_count   | 0         \n",
      " metrotrain_count | 0         \n",
      " metrotram_count  | 0         \n",
      " regbus_count     | 142       \n",
      " regcoach_count   | 2         \n",
      " regtrain_count   | 1         \n",
      " skybus_count     | 0         \n",
      " recr_count       | 1         \n",
      " comm_count       | 1         \n",
      " deal_count       | 739       \n",
      " median_rent      | 350.0     \n",
      " cbd_dis          | 152998.1  \n",
      "only showing top 1 row\n",
      "\n",
      "22/09/20 16:59:18 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , SA2, rent, bedroom, baths, parking, Latitude, Longitude, school_dis, station_dis, cbd_dis\n",
      " Schema: _c0, SA2, rent, bedroom, baths, parking, Latitude, Longitude, school_dis, station_dis, cbd_dis\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/bruce/projects/ass2/data/curated/rent_distance.csv\n",
      "-RECORD 0------------------\n",
      " _c0         | 0           \n",
      " SA2         | 201011001   \n",
      " rent        | 490.0       \n",
      " bedroom     | 4           \n",
      " baths       | 2           \n",
      " parking     | 2           \n",
      " Latitude    | -37.5630731 \n",
      " Longitude   | 143.7938749 \n",
      " school_dis  | 1651.7      \n",
      " station_dis | 5895.5      \n",
      " cbd_dis     | 350.0       \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read SA2 file and create tempview\n",
    "SA2_sdf = spark.read.csv(f\"../data/curated/sa2_info.csv\", header=True)\n",
    "SA2_sdf.show(1, vertical = True, truncate=100)\n",
    "SA2_sdf.createOrReplaceTempView(\"SA2\")\n",
    "\n",
    "# read rent data file and create tempview\n",
    "rent_sdf = spark.read.csv(f\"../data/curated/rent_distance.csv\", header=True)\n",
    "rent_sdf.show(1, vertical = True, truncate=100)\n",
    "rent_sdf.createOrReplaceTempView(\"rent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_c0', 'SA2', 'school_count', 'ERP_population', 'median_income', 'metrobus_count', 'metrotrain_count', 'metrotram_count', 'regbus_count', 'regcoach_count', 'regtrain_count', 'skybus_count', 'recr_count', 'comm_count', 'deal_count', 'median_rent', 'cbd_dis']\n",
      "['_c0', 'SA2', 'rent', 'bedroom', 'baths', 'parking', 'Latitude', 'Longitude', 'school_dis', 'station_dis', 'cbd_dis']\n",
      "-RECORD 0---------------------\n",
      " rent             | 490.0     \n",
      " bedroom          | 4         \n",
      " baths            | 2         \n",
      " parking          | 2         \n",
      " school_dis       | 1651.7    \n",
      " station_dis      | 5895.5    \n",
      " cbd_dis          | 350.0     \n",
      " SA2              | 201011001 \n",
      " school_count     | 4         \n",
      " ERP_population   | 16823     \n",
      " median_income    | 1952      \n",
      " metrobus_count   | 0         \n",
      " metrotrain_count | 0         \n",
      " metrotram_count  | 0         \n",
      " regbus_count     | 43        \n",
      " regcoach_count   | 0         \n",
      " regtrain_count   | 0         \n",
      " skybus_count     | 0         \n",
      " recr_count       | 0         \n",
      " comm_count       | 0         \n",
      "only showing top 1 row\n",
      "\n",
      "['rent', 'bedroom', 'baths', 'parking', 'school_dis', 'station_dis', 'cbd_dis', 'SA2', 'school_count', 'ERP_population', 'median_income', 'metrobus_count', 'metrotrain_count', 'metrotram_count', 'regbus_count', 'regcoach_count', 'regtrain_count', 'skybus_count', 'recr_count', 'comm_count']\n"
     ]
    }
   ],
   "source": [
    "# inner join\n",
    "print(SA2_sdf.columns)\n",
    "print(rent_sdf.columns)\n",
    "combine_sdf = spark.sql(\"\"\"\n",
    "    SELECT  rent, bedroom, baths, parking, school_dis, station_dis, \n",
    "    rent.cbd_dis, median_rent, rent.SA2, school_count, ERP_population, \n",
    "    median_income, metrobus_count, metrotrain_count, metrotram_count, \n",
    "    regbus_count, regcoach_count, regtrain_count, skybus_count, recr_count, comm_count\n",
    "    FROM SA2\n",
    "    INNER JOIN rent ON SA2.SA2 = rent.SA2\n",
    "\"\"\")\n",
    "combine_sdf.show(1, vertical = True, truncate=100)\n",
    "print(combine_sdf.columns)\n",
    "combine_sdf.toPandas().to_csv(\n",
    "    f\"../data/curated/rent_info.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Junhua Liu for study use only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
