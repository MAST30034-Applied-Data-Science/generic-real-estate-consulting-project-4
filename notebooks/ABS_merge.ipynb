{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final outputs:\n",
    "\n",
    "Digital boundary (AUS range, manually select VIC if needed)\n",
    "- ../data/raw/ABS/digitalBoundary/SA2_2021_AUST_GDA2020.shp\n",
    "\n",
    "SA2 code to district names\n",
    "- ../data/raw/ABS/SA2_TO_Name.csv\n",
    "- code(int), name(String)\n",
    "\n",
    "\n",
    "Estimated Resident Population (ERP) (2001 to 2021) (By SA2)\n",
    "- ../data/raw/ABS/ERP/ERP.csv\n",
    "- SA2 (int), year (int), population (int)\n",
    "- SA2: interger code marking each district, e.g. 206041117 for Carlton\n",
    "\n",
    "Household income (weekly) (exclude visitor/non-classifiable) (2021) (By SA2)\n",
    "- ../data/raw/ABS/Household_income/Household_income.csv\n",
    "- SA2 (int), year (int),  household_type (String), income_level (String), popultaion (int)\n",
    "\n",
    "Population projection (2017 - 2066) (VIC overall)\n",
    "- ../data/raw/ABS/Population/Population.csv\n",
    "- year (int), popultaion (int)\n",
    "\n",
    "School location\n",
    "- ../data/raw/ABS/School_location/School_location.csv\n",
    "- School Name (String), SA2 (int), Latitude (float), Longitude (float), School Type (String)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/06 20:50:27 WARN Utils: Your hostname, Bruce-PC resolves to a loopback address: 127.0.1.1; using 172.21.200.124 instead (on interface eth0)\n",
      "22/09/06 20:50:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/06 20:50:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/09/06 20:50:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import re\n",
    "from itertools import compress\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Assignment_2\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "INPUT_DIR = \"../data/raw/ABS/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------\n",
      " SA2          | 209031212 \n",
      " school_count | 9         \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read school file\n",
    "school_sdf = spark.read.csv(f\"{INPUT_DIR}School_location/School_location.csv\", header=True)\n",
    "\n",
    "# count school by SA2\n",
    "school_count = school_sdf.groupBy(\"SA2\").agg({\n",
    "    \"School Name\": \"count\"\n",
    "})\n",
    "school_count = school_count.withColumnRenamed( \"count(School Name)\", \"school_count\")\n",
    "school_count.show(1, vertical = True, truncate=100)\n",
    "school_count.createOrReplaceTempView(\"school\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------\n",
      " SA2        | 201011481 \n",
      " year       | 2010      \n",
      " population | 8664      \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read ERP file and create tempview\n",
    "ERP_sdf = spark.read.csv(f\"{INPUT_DIR}ERP/ERP.csv\", header=True)\n",
    "ERP_sdf.show(1, vertical = True, truncate=100)\n",
    "ERP_sdf.createOrReplaceTempView(\"ERP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------\n",
      " SA2           | 213051589 \n",
      " median_income | 1862      \n",
      "-RECORD 1------------------\n",
      " SA2           | 209041437 \n",
      " median_income | 1979      \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read median household income file and create tempview\n",
    "household_sdf = spark.read.csv(f\"{INPUT_DIR}Household_income/Household_income.csv\", header=True)\n",
    "household_sdf.show(2, vertical = True, truncate=100)\n",
    "household_sdf.createOrReplaceTempView(\"household\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SA2', 'school_count']\n",
      "['SA2', 'year', 'population']\n",
      "['SA2', 'median_income']\n",
      "-RECORD 0-------------------\n",
      " SA2            | 202011018 \n",
      " year           | 2010      \n",
      " school_count   | 13        \n",
      " ERP_population | 14752     \n",
      " median_income  | 1267      \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inner join\n",
    "print(school_count.columns)\n",
    "print(ERP_sdf.columns)\n",
    "print(household_sdf.columns)\n",
    "combine_sdf = spark.sql(\"\"\"\n",
    "SELECT  school.SA2, ERP.year, school.school_count, ERP.population AS ERP_population, median_income\n",
    "FROM school\n",
    "INNER JOIN ERP ON school.SA2 = ERP.SA2\n",
    "INNER JOIN household ON school.SA2 = household.SA2\n",
    "\"\"\")\n",
    "combine_sdf.show(1, vertical = True, truncate=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021']\n"
     ]
    }
   ],
   "source": [
    "print(sorted( combine_sdf.select(F.col('year')).distinct().select(\"year\").rdd.flatMap(lambda x: x).collect() ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
